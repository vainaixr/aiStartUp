{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snippets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "-_nVeZK9vZ8m",
        "nokBXc1CwQ7k",
        "SVFI3iDiw0hA",
        "cHRC5Uqd_KGx",
        "6qj_CkF1gDUD",
        "75fczkzki9Fp",
        "ej3Uwo_smfTs",
        "I6_6BgQuzfuP",
        "Aw1ze6i3zpfR",
        "Wlndqa9tLrGB",
        "r3hEzo0EMcvz",
        "YXnd_IupPf5t",
        "y5Mal_jfXqjw",
        "G-VYs_PvZ5aj",
        "iPT2g2cVeucl",
        "os_xISCFfTU8",
        "2eubf-YzfcJo",
        "8kbPVTSzfesP",
        "U5S9dsaBgaah",
        "jrZvY9Jugo4O",
        "K4EESxOmg0zv",
        "DJmW7gkJg8tj",
        "cXgyVNuXhBzF",
        "Jwwc34-a2F79",
        "NVqByHwi3aMZ",
        "-ovyMKbT81DU",
        "LrwUjNhx0Tfn",
        "O4W5iU2RkhwT",
        "YHPmxCtHqjiV",
        "Ipyk8gJ3AQYE",
        "xfo9ULtNC9E4",
        "G8Q8TZDeDBlh",
        "Eve2ggA_DnPk",
        "vr81DW_iE0mA",
        "XwGErx7gQ2sp",
        "BZdimh1GSA9L",
        "481sMJnMUdGk",
        "-53z9ER2cFT6",
        "5u9Mqc11k3mC",
        "APt8nVpJ9ifZ",
        "hnGiQPDHFFci",
        "JvK-QV83Xblh",
        "uV_5-3qtXge_",
        "ZwxXXuDCXlLN",
        "0AUb1vFTaf0F",
        "-2fOzmLHc_Wp",
        "3l-Ufa8aeq5M",
        "Y4jzJ8sifFqj",
        "hzovOWlXfydt",
        "1TRMIFSR39Lv",
        "-J8M1X5D4Ro3",
        "QeRFhvkI4eKn",
        "O8ryz8J04lhS",
        "N7p9jakt54Ux",
        "tbudFfEBNPV6",
        "UYGVjmKBOZS7",
        "VMA1ea-efEc7",
        "LLb4aCtvPe_W",
        "5lF4okkW4oeI",
        "fovBEoJX4wpE",
        "GqCLNydU45A3",
        "oPTa7Ugq5Eys",
        "mxtjy5Xd5X2A",
        "Sphccv_KKZ89",
        "5dYUs9pwy96L",
        "IYLtkimU7CDM",
        "aAdexZ6Q8Sb2",
        "NYBel5CXCUXy",
        "AxPkHqv7XQgQ",
        "iiyWZtUZXTuo",
        "a3JM77fQakca",
        "idCkpHgwehUt",
        "8IpmmdbVocon",
        "G_eMay8z9psP",
        "Be6H5GGu_TdC",
        "WAAb_J6Z1QGK",
        "sKTnnyQG3RGG",
        "kRFK1vLcFgbB",
        "vCejXsNQKT1O",
        "17VepsDJCcvo",
        "yQli9V0sEC6z",
        "CnIaETSeJbMj",
        "wE0iNEHALs8N",
        "jgWAhMX-OYZ7",
        "ICR09H6OUk3i",
        "N4QbQ4o0UnIN",
        "gTuDmVOCV8Ge",
        "LYvYtY2uZHkk",
        "9lOPQeM-aTTl",
        "OF4qeEYpRJND",
        "7CmE0oadGzb1",
        "mxfpLiJxHiFO",
        "BdXsiGPh_jqm",
        "SO-UuXFINFkL",
        "4rCEJq8tNNoz",
        "qQEvlqeyNfNg",
        "qecglzH4NmRY",
        "MOXl-iCENq5w",
        "TevY7yyNNxsq",
        "vDLssEzX_bFs",
        "t9IkEdP_tGsl",
        "Ml9gdD6288z3",
        "en-wXmHrpy0f",
        "-O4nWxN3kHm_",
        "5FIZNNYglYkr",
        "SyBmRJbGlrr8",
        "E31qKvdirci6",
        "mX3CNsd5-YXv",
        "YnZNVJpfBLM7",
        "GTe9AF-3L8nh",
        "EFTKZkx1Mh_Y",
        "6k7DxCOXIs6N",
        "ZpRYmXxjKHoY",
        "ILRJbyzvKNxj",
        "p9ZjryeuiRAD",
        "d1bTW6N7jHBW",
        "QBrn3pPDpBA4",
        "-9DkivUX3maJ",
        "yxQr_TCM4cEg",
        "0ClXz1Tr5jO3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vainaijr/aiStartUp/blob/master/snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ep1ZiMgDZVb",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz3uc0_UpsFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch-1.0.0a0+1d94a2b-cp36-cp36m-linux_x86_64.whl  \\\n",
        "  http://storage.googleapis.com/pytorch-tpu-releases/tf-1.13/torch_xla-0.1+5622d42-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhqinLNGGmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "pip install --upgrade tb-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRf4rcvlGDsJ",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, unicode_literals, division\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import HTML, Image\n",
        "\n",
        "from pprint import pprint, PrettyPrinter\n",
        "\n",
        "import torch\n",
        "from torch import cat, topk, bmm, stack, norm, zeros, ones, sum, transpose, save, load, manual_seed\n",
        "from torch import from_numpy, rand, randperm, Tensor, zeros_like, log, mean, abs, FloatTensor, mm, matmul\n",
        "from torch import arange, no_grad, mul, LongTensor, argmax, reshape, split, unsqueeze, squeeze, exp, tensor\n",
        "from torch import gather, max, bincount, argsort, multinomial, eye, sqrt, eq, inverse, split, round, min\n",
        "from torch import floor, dot\n",
        "\n",
        "from torch.cuda import is_available, LongTensor, FloatTensor, manual_seed_all \n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from torch.nn.init import kaiming_normal_, orthogonal_, normal_, uniform_, xavier_normal_\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Sequential, ModuleList, ParameterList\n",
        "from torch.nn import Upsample\n",
        "from torch.nn import ReflectionPad2d, ZeroPad2d\n",
        "from torch.nn import Conv1d, Conv2d, ConvTranspose2d, Linear\n",
        "from torch.nn import BatchNorm1d, BatchNorm2d, InstanceNorm2d, LayerNorm\n",
        "from torch.nn import PixelShuffle\n",
        "from torch.nn import Dropout, Dropout2d\n",
        "from torch.nn import LeakyReLU, ReLU, PReLU, Softmax, Tanh, Sigmoid\n",
        "from torch.nn import AdaptiveAvgPool2d, AdaptiveMaxPool2d, MaxPool2d, AvgPool2d, MaxPool1d\n",
        "from torch.nn import RNN, LSTM\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import MSELoss, L1Loss, BCELoss, NLLLoss, BCEWithLogitsLoss, CrossEntropyLoss\n",
        "\n",
        "from torch.nn.functional import relu6, avg_pool2d, softmax, interpolate, linear, conv2d, batch_norm\n",
        "from torch.nn.functional import relu, sigmoid, binary_cross_entropy, log_softmax, nll_loss, normalize\n",
        "from torch.nn.functional import cross_entropy, kl_div, tanh, dropout, cosine_similarity, leaky_relu, mse_loss\n",
        "from torch.nn.functional import conv_transpose2d, avg_pool2d, max_unpool2d, upsample_nearest\n",
        "\n",
        "from torch.autograd import grad, Variable as V\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.models import ResNet, vgg19, vgg19_bn, densenet201, resnet101, resnet34, resnext101_32x8d\n",
        "from torchvision.models import resnet152, resnet18, vgg16 \n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import Resize, TenCrop, RandomResizedCrop\n",
        "from torchvision.transforms import CenterCrop, ColorJitter, RandomCrop, RandomSizedCrop, RandomGrayscale\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomAffine\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "from torch.optim import Adam, SGD, LBFGS\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, MultiStepLR, ReduceLROnPlateau\n",
        "from torch.nn.utils import weight_norm, clip_grad_norm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.checkpoint import checkpoint as cp\n",
        "\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "# import torch_xla\n",
        "# import torch_xla\n",
        "# import torch_xla_py.utils as xu\n",
        "# import torch_xla_py.xla_model as xm\n",
        "import  json, random, re, warnings\n",
        "from json import \n",
        "from string import ascii_lowercase, ascii_uppercase\n",
        "from copy import copy, deepcopy\n",
        "from datetime import timedelta\n",
        "from time import time\n",
        "from glob import glob\n",
        "from os import makedirs, listdir, mkdir\n",
        "from os.path import isdir, join, abspath, dirname, exists\n",
        "from itertools import chain, accumulate, count, tee\n",
        "from functools import partial, namedtuple, reduce\n",
        "from pdb import set_trace \n",
        "\n",
        "from numpy import random, prod, array, asarray, argpartition\n",
        "from numpy import unique, maximum, argwhere, loadtxt, vectorize, minimum\n",
        "\n",
        "import networkx as nx\n",
        "from networkx import Graph, neighbors\n",
        "\n",
        "from numpy.random import normal, randint, permutation, seed, choice\n",
        "\n",
        "from PIL import Image, ImageEnhance, ImageFile\n",
        "from PIL.Image import BICUBIC, open\n",
        "\n",
        "from pandas import DataFrame\n",
        "\n",
        "from collections import deque, defaultdict, OrderedDict\n",
        "\n",
        "from pickle import dump, load\n",
        "\n",
        "from skimage import io, transform\n",
        "\n",
        "from matplotlib import pyplot as plt, ticker as ticker, animation as animation, mlab, pylab\n",
        "\n",
        "import h5py\n",
        "from io import open\n",
        "import tqdm\n",
        "from abc import abstractmethod\n",
        "import scipy.ndimage as nd\n",
        "import seaborn as sbs\n",
        "\n",
        "\n",
        "sbs.set_style('darkgrid')\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.ion() # interactive mode\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['figure.figsize'] = 15, 25\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "writer = SummaryWriter(log_dir='/content/drive/My Drive/runs_/')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boRxq5ML-ypU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06fd821f-3874-4085-a1f2-969ad798ae70"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `scipy.sparse` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXWaH7TKs4M0",
        "colab_type": "text"
      },
      "source": [
        "# Keras imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UboPXBb8s7P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "from keras import optimizers, losses, activations, models\n",
        "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
        "GlobalMaxPool2d, Concatenate, GlobalMaxPooling2d, GlobalAveragePooling2d, Lambda\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import backend as K\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_nVeZK9vZ8m",
        "colab_type": "text"
      },
      "source": [
        "# LeNet model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNqwG3hvcNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nokBXc1CwQ7k",
        "colab_type": "text"
      },
      "source": [
        "# MNIST dataset, dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXZesVXdwVDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataset\n",
        "train_loader = DataLoader(datasets.MNIST('./', train=True, download=True,\n",
        "                                                         transform=Compose([\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize((0.1307,), (0.3081,))\n",
        "                                                         ])), batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# test dataset\n",
        "test_loader = DataLoader(datasets.MNIST('./', train=False, download=False,\n",
        "                                                         transform=Compose([\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize((0.1307,), (0.3081,))\n",
        "                                                         ])), batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "# GAN\n",
        "mnist_loader = DataLoader(datasets.MNIST('./', train=True, download=True,\n",
        "                                                         transform=Compose([\n",
        "                                                             Resize(img_size),\n",
        "                                                             ToTensor(),\n",
        "                                                             Normalize([0.5], [0.5])\n",
        "                                                         ])), batch_size=batch_size, shuffle=True,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFI3iDiw0hA",
        "colab_type": "text"
      },
      "source": [
        "# CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNlpQ5Z1w3dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHRC5Uqd_KGx",
        "colab_type": "text"
      },
      "source": [
        "# transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si7DDADJ_MLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.Resize(imsize),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_transform = Compose([\n",
        "    RandomCrop(200),\n",
        "    RandomHorizontalFlip(),\n",
        "    ColorJitter(),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "data_transforms = {\n",
        "    \n",
        "  'train' : Compose([\n",
        "    RandomResizedCrop(input_size),\n",
        "    RandomHorizontalFlip(),\n",
        "    ColorJitter(),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "  ]),\n",
        "  'val' : Compose([\n",
        "      Resize(input_size),\n",
        "      CenterCrop(input_size),\n",
        "      ToTensor(),\n",
        "      Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ])    \n",
        "}\n",
        "\n",
        "self.x = Omniglot(root, download=True, transform=transforms.Compose([lambda x: Image.open(x).convert('L'),\n",
        "                                                                lambda x: x.resize((imgsz, imgsz)),\n",
        "                                                                lambda x: np.reshape(x, (imgsz, imgsz, 1)),\n",
        "                                                                lambda x: np.transpose(x, [2, 0, 1]),\n",
        "                                                                lambda x: x/255.]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qj_CkF1gDUD",
        "colab_type": "text"
      },
      "source": [
        "# unicodeToAscii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukG38gzUgKJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn a Unicode string to plain ASCII\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75fczkzki9Fp",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zi_uWuOi-Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    combined = torch.cat((input, hidden), 1)\n",
        "    hidden = self.i2h(combined)\n",
        "    output = self.i2o(combined)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej3Uwo_smfTs",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuobQF5Gmhas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6_6BgQuzfuP",
        "colab_type": "text"
      },
      "source": [
        "# Lang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIxw044TzgvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "    self.n_words = 2 # count SOS and EOS\n",
        "    \n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "  \n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1ze6i3zpfR",
        "colab_type": "text"
      },
      "source": [
        "# normalizeString"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZso-1g7zr0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-z.!?]\", r\" \", s)\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlndqa9tLrGB",
        "colab_type": "text"
      },
      "source": [
        "# EncoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y4xjASwLsym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence.\n",
        "# for every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next\n",
        "# input word.\n",
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output = embedded\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3hEzo0EMcvz",
        "colab_type": "text"
      },
      "source": [
        "# DecoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lia56vWzMesF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1, 1, -1)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output[0]))\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnd_IupPf5t",
        "colab_type": "text"
      },
      "source": [
        "# AttnDecoderRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyMbWoVLPhxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.dropout_p = dropout_p\n",
        "    self.max_length = max_length\n",
        "    \n",
        "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.dropout = nn.Dropout(self.dropout_p)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "  \n",
        "  def forward(self, input, hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    embedded = self.dropout(embedded)\n",
        "    \n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "    \n",
        "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "    output = self.attn_combine(output).unsqueeze(0)\n",
        "    \n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    \n",
        "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "    return output, hidden, attn_weights\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Mal_jfXqjw",
        "colab_type": "text"
      },
      "source": [
        "# timeSince"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFS1uBxBXvLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "  es = s / (percent)\n",
        "  rs = es - s\n",
        "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-VYs_PvZ5aj",
        "colab_type": "text"
      },
      "source": [
        "# plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rdlMm4jZ73t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.switch_backend('agg')\n",
        "\n",
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  loc = ticker.MultipleLocator(base=0.2) # this locator puts ticks at regulate intervals\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  plt.plot(points)\n",
        "  \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Test dataset 'Horses'\")\n",
        "plt.imshow(test_A[0])\n",
        "plt.subplot(122)\n",
        "plt.title(\"Test dataset 'Zebras'\")\n",
        "plt.imshow(test_B[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPT2g2cVeucl",
        "colab_type": "text"
      },
      "source": [
        "# BiRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-gWuc4ev-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recurrent Neural Networks (many to one)\n",
        "class BiRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "    super(BiRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "    # torch.nn.LSTM(*args, **kwargs)\n",
        "    # applies a multi layer long shot term memory RNN to an input sequence\n",
        "    \n",
        "    # parameters\n",
        "    # input_size - the number of expected features in the input x\n",
        "    # hidden_size - the number of features in the hidden state h\n",
        "    # num_layers - number of recurrent layers, example, setting num_layers = 2 would mean stacking two LSTMs together\n",
        "    # to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results\n",
        "    # default: 1\n",
        "    # bias - if False, then the layer does not use bias weights, default: True\n",
        "    # batch_first - if True, then the input and output tensors are provided as (batch, seq, feature), default: False\n",
        "    # dropout - if non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer,\n",
        "    # with dropout probability equal to dropout, default: 0\n",
        "    # bidirectional - if True, becomes a bidirectional LSTM, default: False\n",
        "    \n",
        "    # inputs - input, (h_0, c_0)\n",
        "    \n",
        "    # input of shape (seq_len, batch, input_size) - tensor containing the features of the input sequence, the input \n",
        "    # can also be a packed variable length sequence\n",
        "    \n",
        "    # h_0 of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the initial hidden state for \n",
        "    # each element in the batch, if the LSTM is bidirectional, num_directions should be 2, else it should be 1\n",
        "    \n",
        "    # c_0 of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the initial cell state for each\n",
        "    # element in the batch\n",
        "    \n",
        "    # if (h_0, c_0) is not provided, both h_0 and c_0 default to zero\n",
        "    \n",
        "    # outputs - output, (h_n, c_n)\n",
        "    \n",
        "    # output of shape (seq_len, batch, num_directions*hidden_size) - tensor containing the ouput features (h_t) from\n",
        "    # the last layer of the LSTM, for each t\n",
        "    \n",
        "    # h_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the hidden state for \n",
        "    # t = seq_len\n",
        "    \n",
        "    # c_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the cell state for t = seq_len\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size*2, num_classes)\n",
        "    # torch.nn.Linear(in_features, out_features, bias=True)\n",
        "    # applies a linear transformation to the incoming data\n",
        "\n",
        "    # parameters\n",
        "    # in_feature - size of each input sample\n",
        "    # out_feature - size of each output sample\n",
        "    # bias - if set to False, the layer will not learn an additive bias, default: True\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # set initial hidden and cell states\n",
        "    h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
        "    # torch.zeros(*sizes, out=None, layout=torch.strided, device=None, requires_grad=False)\n",
        "    # returns a tensor filled with the scalar value 0, with the shape defined by the variable argument sizes\n",
        "\n",
        "    # parameters\n",
        "    # sizes - a sequence of integers defining the shape of the output tensor, can be a variable number of argumentss\n",
        "    # or a collection like a list or tuple\n",
        "    # out - the output tensor\n",
        "    # dtype - the desired data type of returned tensor, default: if None, uses a global default\n",
        "    # layout - the desired layout of returned tensor, default: torch.strided\n",
        "    # device - the desired device of returned tensor\n",
        "    # requires_grad - if autograd should record operations on the returned tensor, default: False\n",
        "\n",
        "    c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size)\n",
        "\n",
        "    # forward propagate LSTM\n",
        "    out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "    # outputs - output, (h_n, c_n)\n",
        "\n",
        "    # output of shape (seq_len, batch, num_directions*hidden_size) - tensor containing the ouput features (h_t) from\n",
        "    # the last layer of the LSTM, for each t\n",
        "\n",
        "    # h_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the hidden state for \n",
        "    # t = seq_len\n",
        "\n",
        "    # c_n of shape (num_layers*num_directions, batch, hidden_size) - tensor containing the cell state for \n",
        "    # t = seq_len\n",
        "\n",
        "    # decode the hidden state of the last time step\n",
        "    out = self.fc(out[:, -1, :])\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os_xISCFfTU8",
        "colab_type": "text"
      },
      "source": [
        "# loss, optimizer and learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDevrOldfVQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # loss function, example, MSELoss, L1Loss, CTCLoss, NLLLoss, PoissonNLLLoss, KLDivLoss,BCELoss,\n",
        "# BCEWithLogitsLoss, MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, \n",
        "# MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params, lr=learning_rate) # optimizer, example, Adadelta, Adagrad, SparseAdam, Adamax,\n",
        "# ASGD, LBFGS, RMSprop, Rprop, SGD\n",
        "\n",
        "# and a learning rate scheduler which decrease the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eubf-YzfcJo",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kwAGuDfdQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        # reshape(*shape) -> Tensor\n",
        "        # returns a tensor with the same data and number of elements as self but with the specified shape.\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # backward and optimize\n",
        "        optimizer.zero_grad() # set gradients of all model parameters to zero\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kbPVTSzfesP",
        "colab_type": "text"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNVQ5H_zffkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        \n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        \n",
        "        # torch.max(input, dim, keepdim=False, out=None) returns a namedtuple (values, indices) where values is the maximum\n",
        "        # value of each row of the input tensor in the given dimension dim, and indices is the index location of each maximum\n",
        "        # value found (argmax).\n",
        "        # If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of\n",
        "        # size 1\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item() \n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100*correct / total))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5S9dsaBgaah",
        "colab_type": "text"
      },
      "source": [
        "# make_grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pqEXKI0gbw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image):\n",
        "  if isinstance(image, torch.Tensor):\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "  else:\n",
        "    image = np.array(image).transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  image = std * image + mean\n",
        "  image = np.clip(image, 0, 1)\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "  plt.imshow(image)\n",
        "  ax.axis('off')\n",
        "\n",
        "images, _ = next(iter(train_loader))\n",
        "out = make_grid(images, nrow=8)\n",
        "imshow(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZvY9Jugo4O",
        "colab_type": "text"
      },
      "source": [
        "# Ignite import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I9H854Igqjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignite is a high level library to help with training neural networks in PyTorch, it comes with an Engine to setup\n",
        "# a training loop, various metrics, handlers\n",
        "!pip install ignite\n",
        "from ignite.engine import Engine, Events \n",
        "# Engine - runs a given process_function over each batch of a dataset, emitting events as it goes\n",
        "# Events - allows users to attach functions to an Engine to fire functions at a specific event, example: \n",
        "# EPOCH_COMPLETED, ITERATION_STARTED etc.\n",
        "\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "# Accuracy - metric to calculate accuracy over a dataset, for binary, multiclass, multilabel cases\n",
        "# Loss - general metric that takes a loss function as a parameter, calculate loss over a dataset\n",
        "# RunningAverage - general metric to attach to Engine during training\n",
        "\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "# ModelCheckpoint - handler to checkpoint models\n",
        "# EarlyStopping - handler to stop training based on a score function\n",
        "\n",
        "from ignite.contrib.handlers import ProgressBar\n",
        "# ProgressBar - handler to create a tqdm progress bar, tqdm means progress in Arabic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EESxOmg0zv",
        "colab_type": "text"
      },
      "source": [
        "# TextCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK75h1Meg2Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TextCNN model\n",
        "# the model works for variable text lengths, we embed the words of a sentence, use convolutions, maxpooling\n",
        "# and concatenation to embed the sentence as a single vector\n",
        "# the single vector is passed through a fully connected layer with sigmoid to output a single value\n",
        "# this value can be interpreted as the probability a sentence is positive (closer to 1) or negative (closer to 0)\n",
        "\n",
        "# the minimum length of text expected by the model is the size of the smallest kernel size of the model\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes, d_prob, mode):\n",
        "    super(TextCNN, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.kernel_sizes = kernel_sizes\n",
        "    self.num_filters = num_filters\n",
        "    self.num_classes = num_classes\n",
        "    self.d_prob = d_prob\n",
        "    self.mode = mode\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.load_embeddings()\n",
        "    self.conv = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k,\n",
        "                                        stride=1) for k in kernel_sizes])\n",
        "    self.dropout = nn.Dropout(d_prob)\n",
        "    self.fc = nn.Linear(len(kernel_sizes)*num_filters, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size, sequence_length = x.shape\n",
        "    x = self.embedding(x).transpose(1, 2)\n",
        "    x = [F.relu(conv(x)) for conv in self.conv]\n",
        "    x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]\n",
        "    x = torch.cat(x, dim=1)\n",
        "    x = self.fc(self.dropout(x))\n",
        "    return torch.sigmoid(x).squeeze()\n",
        "  \n",
        "  def load_embeddings(self):\n",
        "    if 'static' in self.mode:\n",
        "      self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "      if 'non' not in self.mode:\n",
        "        self.embedding.weight.data.requires_grad = False\n",
        "        print('Loaded pretrained embeddings, weights are not trainable')\n",
        "      else:\n",
        "        self.embedding.weight.data.requires_grad = True\n",
        "        print('Loaded pretrained embeddings, weights are trainable')\n",
        "      \n",
        "    elif self.mode == 'rand':\n",
        "      print('Randoml initialized embeddings are used')\n",
        "    else:\n",
        "      raise ValueError('Unexpected value of mode')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJmW7gkJg8tj",
        "colab_type": "text"
      },
      "source": [
        "# Ignite process_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr7oIZEUg-4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training and evaluating using Ignite\n",
        "# Ignite's Engine allows user to define a process_function a given batch, this is applied to all the batches of \n",
        "# the dataset, this is a general class that can be applied to train and validate models, a process_function\n",
        "# has two parameters, engine and batch\n",
        "\n",
        "# the function of the trainer\n",
        "# sets model in train mode\n",
        "# sets the gradient of the optimizer to zero\n",
        "# generate x and y from batch\n",
        "# performs a forward pass to calculate y_pred using model and x\n",
        "# calculates loss using y_pred and y\n",
        "# performs a backward pass using loss to calculate gradients for the model parameters\n",
        "# model parameters are optimized using gradients and optimizer\n",
        "# returns scalar loss\n",
        "\n",
        "def process_function(engine, batch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  x, y = batch.text, batch.label\n",
        "  y_pred = model(x)\n",
        "  loss = criterion(y_pred, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXgyVNuXhBzF",
        "colab_type": "text"
      },
      "source": [
        "# Ignite eval_function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8ZoZE-IhDhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluator engine - process_function\n",
        "\n",
        "# similar to the training process function, we setup a function to evaluate a single batch\n",
        "\n",
        "# eval_function\n",
        "# sets model in eval mode\n",
        "# generates x and y from batch\n",
        "# with torch.no_grad(), no gradients are calculated for any succeeding steps\n",
        "# performs a forward pass on the model to calculate y_pred based on model and x\n",
        "# returns y_pred and y\n",
        "\n",
        "# Ignite suggests attaching metrics to evaluators and not trainers\n",
        "\n",
        "# all metrics in Ignite require y_pred and y as outputs of the function attached to the Engine\n",
        "\n",
        "def eval_function(engine, batch):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    x, y = batch.text, batch.label\n",
        "    y_pred = model(x)\n",
        "    return y_pred, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwwc34-a2F79",
        "colab_type": "text"
      },
      "source": [
        "# CustomDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDfZeD4U2Iol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, transforms_x=None, transforms_lr=None, mode='train'):\n",
        "    self.transform_x = transforms.Compose(transforms_x)\n",
        "    self.transform_lr = transforms.Compose(transforms_lr)\n",
        "    self.files = sorted(glob.glob('%s/*.*' % root))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    \n",
        "    x = self.transform_x(img)\n",
        "    x_lr = self.transform_lr(img)\n",
        "    \n",
        "    return {'x': x, 'x_lr': x_lr}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, input_shape, mode=\"train\"):\n",
        "    self.transform = transforms.Compose([\n",
        "        transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "    \n",
        "    self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    w, h = img.size\n",
        "    img_A = img.crop((0, 0, w / 2, h))\n",
        "    img_B = img.crop((w / 2, 0, w, h))\n",
        "    \n",
        "    if np.random.random() < 0.5:\n",
        "      img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "      img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "    \n",
        "    img_A = self.transform(img_A)\n",
        "    img_B = self.transform(img_B)\n",
        "    \n",
        "    return {\"A\": img_A, \"B\": img_B}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "  \n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n",
        "    self.transform = transforms.Compose(transforms_)\n",
        "    self.img_size = img_size\n",
        "    self.mask_size = mask_size\n",
        "    self.mode = mode\n",
        "    self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n",
        "    self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n",
        "  \n",
        "  def apply_random_mask(self, img):\n",
        "    \"\"\"\n",
        "    randomly masks image\n",
        "    \"\"\"\n",
        "    y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
        "    y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
        "    masked_part = img[:, y1:y2, x1:x2]\n",
        "    masked_img = img.clone()\n",
        "    masked_img[:, y1:y2, x1:x2] = 1\n",
        "    \n",
        "    return masked_img, masked_part\n",
        "  \n",
        "  def apply_center_mask(self, img):\n",
        "    \"\"\"\n",
        "    mask center part of image\n",
        "    \"\"\"\n",
        "    # get upper-left pixel coordinate\n",
        "    i = (self.img_size - self.mask_size) // 2\n",
        "    masked_img = img.clone()\n",
        "    masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
        "    \n",
        "    return masked_img, i\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    \n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    img = self.transform(img)\n",
        "    if self.mode == \"train\":\n",
        "      # for training data perform random mask\n",
        "      masked_img, aux = self.apply_random_mask(img)\n",
        "    else:\n",
        "      # for test data mask the center of the image\n",
        "      masked_img, aux = self.apply_center_mask(img)\n",
        "    \n",
        "    return img, masked_img, aux\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
        "    self.transform = transforms.Compose(transforms_)\n",
        "    self.unaligned = unaligned\n",
        "    \n",
        "    self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n",
        "    self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "    \n",
        "    if self.unaligned:\n",
        "      image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "    else:\n",
        "      image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "    \n",
        "    # convert grayscale images to rgb\n",
        "    if image_A.mode != \"RGB\":\n",
        "      image_A = to_rgb(image_A)\n",
        "    if image_B.mode != \"RGB\":\n",
        "      image_B = to_rgb(image_B)\n",
        "    \n",
        "    item_A = self.transform(image_A)\n",
        "    item_B = self.transform(image_B)\n",
        "    \n",
        "    return {\"A\": item_A, \"B\": item_B}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return max(len(self.files_A), len(self.files_B))\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, hr_shape):\n",
        "    hr_height, hr_width = hr_shape\n",
        "    \n",
        "    # transforms for low resolution images and high resolution images\n",
        "    self.lr_transform = transforms.Compose([\n",
        "        transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),        \n",
        "    ])\n",
        "    \n",
        "    self.hr_transform = transforms.Compose([\n",
        "        transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    self.files = sorted(glob.glob(root + \"/*.*\"))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    img_lr = self.lr_transform(img)\n",
        "    img_hr = self.hr_transform(img)\n",
        "    \n",
        "    return {\"lr\": img_lr, \"hr\": img_hr}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "  \n",
        "class PennFudanDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "      \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return 0\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return None\n",
        "  \n",
        "class FaceLandmarksDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    # csv_file (string): path to the csv file with annotations.\n",
        "    # root_dir (string): directory with all the images.\n",
        "    # transform (callable, optional): optional transform to be applied on a sample.\n",
        "    \n",
        "    self.landmarks_frame = pd.read_csv(csv_file)\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.landmarks_frame)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, 0])\n",
        "    image = io.imread(img_name)\n",
        "    landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
        "    landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "    sample = {'image': image, 'landmarks': landmarks}\n",
        "    \n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    \n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVqByHwi3aMZ",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo_nLRFV3cw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training helpers\n",
        "\n",
        "# list(get_trainable(model.parameters()))\n",
        "def get_trainable(model_params):\n",
        "  return (p for p in model_params if p.requires_grad)\n",
        "\n",
        "# list(get_frozen(model.parameters()))\n",
        "def get_frozen(model_params):\n",
        "  return (p for p in model_params if not p.requires_grad)\n",
        "\n",
        "# all_trainable(model.parameters())\n",
        "def all_trainable(model_params):\n",
        "  return all(p.requires_grad for p in model.params)\n",
        "\n",
        "# all_frozen(model.parameters())\n",
        "def all_frozen(model_params):\n",
        "  return all(not p.requires_grad for p in model.params)\n",
        "\n",
        "def freeze_all(model_params):\n",
        "  for param in model_params:\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ovyMKbT81DU",
        "colab_type": "text"
      },
      "source": [
        "# pretrainedmodels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUGpT8V823d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pretrainedmodels\n",
        "import pretrainedmodels\n",
        "print(pretrainedmodels.model_names)\n",
        "\n",
        "model_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2\n",
        "model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrwUjNhx0Tfn",
        "colab_type": "text"
      },
      "source": [
        "# train_model_helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QE_9BGv0XIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "  since = time.time()\n",
        "  \n",
        "  val_acc_history = []\n",
        "  \n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train() # set model to training mode\n",
        "      else:\n",
        "        model.eval() # set model to eval mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # iterate over data\n",
        "      for inputs, labels in dataloader[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          # get model outputs and calculate loss\n",
        "          # special case for inception because in training it has an auxiliary output.\n",
        "          # in train mode we calculate the loss by summing the final output and the auxiliary output but in testing\n",
        "          # we only consider the final output.\n",
        "          if is_inception and phase == 'train':\n",
        "            outputs, aux_outputs = model(inputs)\n",
        "            loss1 = criterion(outputs, labels)\n",
        "            loss2 = criterion(aux_outputs, labels)\n",
        "            loss = loss1 + 0.4*loss2\n",
        "          else:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    # deep copy the model\n",
        "    if phase == 'val' and epoch_acc > best_acc:\n",
        "      best_acc = epoch_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if phase == 'val':\n",
        "      val_acc_history.append(epoch_acc)\n",
        "\n",
        "  print()\n",
        "  \n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapse // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "  \n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, val_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4W5iU2RkhwT",
        "colab_type": "text"
      },
      "source": [
        "# datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmyZmuDkjzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train',\n",
        "                                                                                                   'val']}\n",
        "\n",
        "# create training and vaildation dataloaders\n",
        "dataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in\n",
        "                  ['train', 'val']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHPmxCtHqjiV",
        "colab_type": "text"
      },
      "source": [
        "# plot_compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq6fax2mqkr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1, num_epochs+1), ohist, label=\"Pretrained\")\n",
        "plt.plot(range(1, num_epochs+1), shist, label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipyk8gJ3AQYE",
        "colab_type": "text"
      },
      "source": [
        "# STN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeyUfA7bAR1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    # Spatial transformer localization-network\n",
        "    self.localization = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, kernel_size=7),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(8, 10, kernel_size=5),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    \n",
        "    # regressor for the 3 * 2 affine matrix\n",
        "    self.fc_loc = nn.Sequential(\n",
        "        nn.Linear(10 * 3 * 3, 32),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(32, 3 * 2)\n",
        "    )\n",
        "    \n",
        "    # initialize the weights/bias with identity transformation\n",
        "    self.fc_loc[2].weight.data.zero_()\n",
        "    self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "    \n",
        "  # spatial transformer network forward function\n",
        "  def stn(self, x):\n",
        "    xs = self.localization(x)\n",
        "    xs = xs.view(-1, 10 * 3 * 3)\n",
        "    theta = self.fc_loc(xs)\n",
        "    theta = theta.view(-1, 2, 3)\n",
        "    \n",
        "    grid = F.affine_grid(theta, x.size())\n",
        "    x = F.grid_sample(x, grid)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # transform the input\n",
        "    x = self.stn(x)\n",
        "    \n",
        "    # perform forward pass\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "  \n",
        "model = Net().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfo9ULtNC9E4",
        "colab_type": "text"
      },
      "source": [
        "# train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIrTorbiC-lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  # total_step = len(train_loader)\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    # move tensors to the configured device\n",
        "    images = images.to(device)\n",
        "    # reshape(*shape) -> Tensor\n",
        "    # returns a tensor with the same data and number of elements as self but with the specified shape.\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(images)\n",
        "    loss = F.nll_loss(outputs, labels)\n",
        "\n",
        "    # backward and optimize\n",
        "    optimizer.zero_grad() # set gradients of all model parameters to zero\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx) % 500 == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(images), \n",
        "                                                                       len(train_loader.dataset), \n",
        "                                                                       100. * batch_idx / len(train_loader), \n",
        "                                                                       loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Q8TZDeDBlh",
        "colab_type": "text"
      },
      "source": [
        "# test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEdBjQeCDC3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1) \n",
        "\n",
        "          test_loss += F.nll_loss(outputs, labels, size_averag=False).item()\n",
        "\n",
        "          pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      \n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      \n",
        "      print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct,\n",
        "                                                                                  len(test_loader.dataset),\n",
        "                                                                                  100. * correct / \n",
        "                                                                                  len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eve2ggA_DnPk",
        "colab_type": "text"
      },
      "source": [
        "# tensor to numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es-6eB4bDo0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_image_np(inp):\n",
        "  inp = inp.numpy().transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  inp = std * inp + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  return inp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr81DW_iE0mA",
        "colab_type": "text"
      },
      "source": [
        "# visualize stn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8yjrepRE19Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_stn():\n",
        "  with torch.no_grad():\n",
        "    # get a batch of training data\n",
        "    data = next(iter(test_loader))[0].to(device)\n",
        "    \n",
        "    input_tensor = data.cpu()\n",
        "    transformed_input_tensor = model.stn(data).cpu()\n",
        "    \n",
        "    in_grid = convert_image_np(make_grid(input_tensor))\n",
        "    \n",
        "    out_grid = convert_image_np(make_grid(transformed_input_tensor))\n",
        "    \n",
        "    # plot the results side by side\n",
        "    f, axarr = plt.subplots(1, 2)\n",
        "    axarr[0].imshow(in_grid)\n",
        "    axarr[0].set_title('Dataset Images')\n",
        "    \n",
        "    axarr[1].imshow(out_grid)\n",
        "    axarr[1].set_title('Transformed Images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwGErx7gQ2sp",
        "colab_type": "text"
      },
      "source": [
        "# image loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2nDpfRHQ4Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_loader(image_name):\n",
        "  image = Image.open(image_name)\n",
        "  # fake batch dimension required to fit network's input dimensions\n",
        "  image = transforms(image).unsqueeze(0)\n",
        "  return image.to(device, torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZdimh1GSA9L",
        "colab_type": "text"
      },
      "source": [
        "# tensor to PIL show"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkZ5pj_YSCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unloader = ToPILImage() # reconvert into PIL image\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "  image = tensor.cpu().clone() # we clone the tensor to not do changes on it\n",
        "  image = image.squeeze(0)\n",
        "  image = unloader(image)\n",
        "  plt.imshow(image)\n",
        "  if title is not None:\n",
        "    plt.title(title)\n",
        "  plt.pause(0.001) # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "imshow(style_img, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "481sMJnMUdGk",
        "colab_type": "text"
      },
      "source": [
        "# gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omw6AIm-UfR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input):\n",
        "  a, b, c, d = input.size() \n",
        "  # a = batch size(=1)\n",
        "  # b = number of feature maps\n",
        "  # (c, d) = dimensions of a f. map (N=c*d)\n",
        "  \n",
        "  features = input.view(a * b, c * d)\n",
        "  \n",
        "  G = torch.mm(features, features.t()) # compute the gram product \n",
        "  \n",
        "  # we 'normalize' the values of the gram matrix by dividing by the number of element in each feature maps.\n",
        "  return G.div(a * b * c * d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-53z9ER2cFT6",
        "colab_type": "text"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvOOT1gHcHWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a module to normalize input image so we can easily put it in a nn.Sequential\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalization, self).__init__()\n",
        "    # view the mean and std to make them [c x 1 x] so that they can directly work with image Tensor of shape\n",
        "    # [B x C x H x W].\n",
        "    # B is batch size.\n",
        "    # C is number of channels.\n",
        "    # H is height and W is width\n",
        "    self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "  \n",
        "  def forward(self, img):\n",
        "    # normalize img\n",
        "    return (img - self.mean) / self.std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u9Mqc11k3mC",
        "colab_type": "text"
      },
      "source": [
        "# FGSM attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zazsh7Dxk6Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "  # collect the element-wise sign of the data gradient\n",
        "  sign_data_grad = data_grad.sign()\n",
        "  \n",
        "  # create the perturbed image by adjusting each pixel of the input image\n",
        "  perturbed_image = image + epsilon * sign_data_grad\n",
        "  \n",
        "  # adding clipping to maintain [0, 1] range\n",
        "  perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "  \n",
        "  return perturbed_image\n",
        "\n",
        "# fgsm attack test\n",
        "def test(model, device, test_loader, epsilon):\n",
        "  \n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  adv_examples = []\n",
        "  for data, target in test_loader:\n",
        "\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      data.requires_grad = True\n",
        "      \n",
        "      output = model(data)\n",
        "\n",
        "      init_pred = output.max(1, keepdim=True)[1]\n",
        "      \n",
        "      if init_pred.item() != target.item(): # if the initial prediction is wrong, do not bother attacking\n",
        "        continue\n",
        "        \n",
        "      loss = F.nll_loss(output, target)\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      # collect datagrad\n",
        "      data_grad = data.grad.data\n",
        "      \n",
        "      # call FGSM attack\n",
        "      perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
        "      \n",
        "      # reclassify the perturbed image\n",
        "      output = model(perturbed_data)\n",
        "      \n",
        "      # check for success\n",
        "      final_pred = output.max(1, keepdim=True)[1] # get the index of the max log probability\n",
        "      if final_pred.item() == target.item():\n",
        "        correct += 1\n",
        "        if (epsilon == 0) and (len(adv_examples) < 5):\n",
        "          # special case for saving 0 epsilon examples\n",
        "          adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "          adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
        "        else:\n",
        "          # save some adv examples for visualization later\n",
        "          if len(adv_examples) < 5:\n",
        "            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "            adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
        "  \n",
        "  \n",
        "  # calculate final accuracy for this epsilon\n",
        "  final_acc = correct/float(len(test_loader))\n",
        "\n",
        "  print('Epsilon: {}\\t, Test Accuracy = {} / {} = {}'.format(epsilon, correct,\n",
        "                                                                              len(test_loader),\n",
        "                                                                              final_acc))\n",
        "  # return the accuracy and an adversarial example\n",
        "  return final_acc, adv_examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APt8nVpJ9ifZ",
        "colab_type": "text"
      },
      "source": [
        "# show image with landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayEVnAEX9lL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show image with landmarks\n",
        "def show_landmarks(image, landmarks):\n",
        "  plt.imshow(image)\n",
        "  plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "  plt.pause(0.001) # pause a bit so that plots are updated\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('faces/', img_name)), landmarks)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnGiQPDHFFci",
        "colab_type": "text"
      },
      "source": [
        "# CustomTransforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAXEjB2mFISv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  Rescale the image in a sample to a given size.\n",
        "  \n",
        "  Args:\n",
        "    output_size (tuple or int): desired output size.\n",
        "    if tuple, output is matched to output_size.\n",
        "    if int, smaller of image edges is matched to output_size keeping aspect ratio the same.\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    \n",
        "    h, w = image.shape[:2]\n",
        "    \n",
        "    if isinstance(self.output_size, int):\n",
        "      if h > w:\n",
        "        new_h, new_w = self.output_size * h / w, self.output_size\n",
        "      else:\n",
        "        new_h, new_w = self.output_size, self.output_size * w / h\n",
        "    else:\n",
        "      new_h, new_w = int(new_h), int(new_w)\n",
        "      \n",
        "    img = transform.resize(image, (new_h, new_w))\n",
        "    \n",
        "    # h and w are swapped for landmarks because for images, x and y axes are axis 1 and 0 respectively\n",
        "    \n",
        "    landmarks = landmarks * [new_w / w, new_h / h]\n",
        "    \n",
        "    return {'image': img, 'landmarks': landmarks}\n",
        "\n",
        "class RandomCrop(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  Crop randomly the image in a sample.\n",
        "  \n",
        "  Args:\n",
        "    output_size (tuple or int): desired output size.\n",
        "    if int, square crop is made.\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_size):\n",
        "    assert isinstance(output_size, (int, tuple))\n",
        "    if isinstance(output_size, int):\n",
        "      self.output_size = (output_size, output_size)\n",
        "    else:\n",
        "      assert len(output_size) == 2\n",
        "      self.output_size = output_size\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    h, w = iamge.shape[:2]\n",
        "    new_h, new_w = self.output_size\n",
        "    \n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "    \n",
        "    image = image[top: top + new_h, left: left + new_w]\n",
        "    \n",
        "    landmarks = landmarks - [left, top]\n",
        "    \n",
        "    return {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "class ToTensor(object):\n",
        "  \"\"\"\n",
        "  \n",
        "  convert ndarrays in sample to tensors.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __call__(self, sample):\n",
        "    image, landmarks = sample['image'], sample['landmarks']\n",
        "    \n",
        "    # swap color axis because\n",
        "    # numpy image : H x W x C\n",
        "    # torch image : C x H x w\n",
        "    \n",
        "    image = image.transpose((2, 0, 1))\n",
        "    \n",
        "    return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvK-QV83Xblh",
        "colab_type": "text"
      },
      "source": [
        "# sparsity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjQvd6F1Xc2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparsity(cl_data_file):\n",
        "  class_list = cl_data_file.keys()\n",
        "  cl_sparsity = []\n",
        "  for cl in class_list:\n",
        "    cl_sparsity.append(np.mean([np.sum(x!=0) for x in cl_data_file[cl]]))\n",
        "    \n",
        "  return np.mean(cl_sparsity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_5-3qtXge_",
        "colab_type": "text"
      },
      "source": [
        "# one_hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-bz0c9sXiTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(y, num_class):\n",
        "  return torch.zeros((len(y), num_class)).scatter_(1, y.unsqueeze(1), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwxXXuDCXlLN",
        "colab_type": "text"
      },
      "source": [
        "# DBindex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBXyx6NuXmyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DBindex(cl_data_file):\n",
        "  class_list = cl_data_file.keys()\n",
        "  cl_num = len(class_list)\n",
        "  cl_means = []\n",
        "  stds = []\n",
        "  DBs = []\n",
        "  for cl in class_list:\n",
        "    cl_means.append(np.mean(cl_data_file[cl], axis=0))\n",
        "    stds.append(np.sqrt(np.mean(np.sum(np.square(cl_data_file[cl] - cl_means[-1]), axis = 1))))\n",
        "    \n",
        "  mu_i = np.tile(np.expand_dims(np.array(cl_means), axis = 0), len(class_list), 1, 1)\n",
        "  mu_j = np.transpose(mu_i, (1, 0, 2))\n",
        "  mdists = np.sqrt(np.sum(np.square(mu_i - mu_j), axis = 2))\n",
        "  \n",
        "  for i in range(cl_num):\n",
        "    DBs.append(np.max([(stds[i] + stds[j])/mdists[i, j] for j in range(cl_num) if j != i]))\n",
        "  return np.mean(DBs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUb1vFTaf0F",
        "colab_type": "text"
      },
      "source": [
        "# CustomTransforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDLROP4PahkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformLoader:\n",
        "  def __init__(self, image_size, normalize_param = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "               jitter_param=dict(Brightness=0.4, Contrast=0.4, Color=0.4)):\n",
        "    self.image_size = image_size\n",
        "    self.normalize_param = normalize_param\n",
        "    self.jitter_param = jitter_param\n",
        "    \n",
        "  def parse_transform(self, transform_type):\n",
        "    if transform_type == 'ImageJitter':\n",
        "      method = add_transforms.ImageJitter(self.jitter_param)\n",
        "      return method\n",
        "    method = getattr(transforms, transform_type)\n",
        "    if transform_type == 'RandomSizedCrop':\n",
        "      return method(self.image_size)\n",
        "    elif transform_type == 'CenterCrop':\n",
        "      return method(self.image_size)\n",
        "    elif transform_type == 'Scale':\n",
        "      return method([int(self.image_size*1.15), int(self.image_size*1.15)])\n",
        "    elif transform_type == 'Normalize':\n",
        "      return method(**self.normalize_param)\n",
        "    else:\n",
        "      return method()\n",
        "    \n",
        "  def get_composed_transform(self, aug = False):\n",
        "    if aug:\n",
        "      transform_list = ['RandomSizedCrop', 'ImageJitter', 'RandomHorizontalFlip', 'ToTensor', 'Normalize']\n",
        "    else:\n",
        "      transform_list = ['Scale', 'CenterCrop', 'ToTensor', 'Normalize']\n",
        "    \n",
        "    transform_funcs = [self.parse_transform(x) for x in transform_list]\n",
        "    transform = transforms.Compose(transform_funcs)\n",
        "    return transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fOzmLHc_Wp",
        "colab_type": "text"
      },
      "source": [
        "# SimpleDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_PggqqRdAmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleDataset:\n",
        "  def __init__(self, data_file, transform, target_transform=identity):\n",
        "    with open(data_file, 'r') as f:\n",
        "      self.meta = json.load(f)\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.meta['image_names'][i])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    target = self.target_transform(self.meta['image_labels'])\n",
        "    return img, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.meta['image_names'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l-Ufa8aeq5M",
        "colab_type": "text"
      },
      "source": [
        "# SetDataset, SubDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-NO50Tlet7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SetDataset:\n",
        "  def __init__(self, data_file, batch_size, transform):\n",
        "    with open(data_file, 'r') as f:\n",
        "      self.meta = json.load(f)\n",
        "      \n",
        "    self.cl_list = np.unique(self.meta['image_labels']).tolist()\n",
        "    \n",
        "    self.sub_meta = {}\n",
        "    \n",
        "    for cl in self.cl_list:\n",
        "      self.sub_meta[cl] = []\n",
        "      \n",
        "    for x, y in zip(self.meta['image_names'], self.meta['image_labels']):\n",
        "      self.sub_meta[y].append(x)\n",
        "    \n",
        "    self.sub_dataloader = []\n",
        "    \n",
        "    # use main thread only or may receive multiple batches\n",
        "    sub_data_loader_params = dict(batch_size = batch_size, shuffle = True, num_workers = 0, pin_memory = False)\n",
        "    \n",
        "    for cl in self.cl_list:\n",
        "      sub_dataset = SubDataset(self.sub_meta[cl], cl, transform = transform)\n",
        "      self.sub_dataloader.append(DataLoader(sub_dataset, **sub_data_loader_params))\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return next(iter(self.sub_dataloader[i]))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.cl_list)\n",
        "  \n",
        "class SubDataset:\n",
        "  def __init__(self, sub_meta, cl, transform=transforms.ToTensor(), target_transform=identity):\n",
        "    self.sub_meta = sub_meta\n",
        "    self.cl = cl\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    image_path = os.path.join(self.sub_meta[i])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    target = self.target_transform(self.cl)\n",
        "    return img, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.sub_meta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4jzJ8sifFqj",
        "colab_type": "text"
      },
      "source": [
        "# EpisodicBatchSampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m57wRwGfIAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpisodicBatchSampler(object):\n",
        "  def __init__(self, n_classes, n_way, n_episodes):\n",
        "    self.n_classes = n_classes\n",
        "    self.n_way = n_way\n",
        "    self.n_episodes = n_episodes\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.n_episodes\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for i in range(self.n_episodes):\n",
        "      yield torch.randperm(self.n_classes)[:self.n_way]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzovOWlXfydt",
        "colab_type": "text"
      },
      "source": [
        "# SimpleHDF5Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWnkwG5f0mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature loader\n",
        "\n",
        "class SimpleHDF5Dataset:\n",
        "  def __init__(self, file_handle = None):\n",
        "    if file_handle == None:\n",
        "      self.f = ''\n",
        "      self.all_feats_dset = []\n",
        "      self.all_labels = []\n",
        "      self.total = 0\n",
        "    else:\n",
        "      self.f = file_handle\n",
        "      self.all_feats_dset = self.f['all_feats'][...]\n",
        "      self.all_labels = self.f['all_labels'][...]\n",
        "      self.total = self.f['count'][0]\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return torch.Tensor(self.all_feats_dset[i, :], int(self.all_labels[i]))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.total\n",
        "  \n",
        "def init_loader(filename):\n",
        "  with h5py.File(filename, 'r') as f:\n",
        "    fileset = SimpleHDF5Dataset(f)\n",
        "  \n",
        "  feats = fileset.all_feats_dset\n",
        "  labels = fileset.all_labels\n",
        "  while np.sum(feats[-1]) == 0:\n",
        "    feats = np.delete(feats, -1, axis = 0)\n",
        "    labels = np.delete(labels, -1, axis = 0)\n",
        "  \n",
        "  class_list = np.unique(np.array(labels)).tolist()\n",
        "  inds = range(len(labels))\n",
        "  \n",
        "  cl_data_file = {}\n",
        "  for cl in class_list:\n",
        "    cl_data_file[cl] = []\n",
        "  for ind in inds:\n",
        "    cl_data_file[labels[ind]].append(feats[ind])\n",
        "  \n",
        "  return cl_data_file\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRMIFSR39Lv",
        "colab_type": "text"
      },
      "source": [
        "# multiple models save, load\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrNmfVqG3_V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save({\n",
        "    'modelA_state_dict': modelA.state_dict(),\n",
        "    'modelB_state_dict': modelB.state_dict(),\n",
        "    'optimizerA_state_dict': optimizerA.state_dict(),\n",
        "    'optimizerB_state_dict': optimizerB.state_dict(),\n",
        "    ...\n",
        "}, PATH)\n",
        "\n",
        "# load\n",
        "modelA = TheModelAClass(*args, **kwargs)\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
        "optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "modelA.eval()\n",
        "modelB.eval()\n",
        "# or\n",
        "modelA.train()\n",
        "modelB.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J8M1X5D4Ro3",
        "colab_type": "text"
      },
      "source": [
        "# saving and loading a general checkpoint for inference and/or resuming training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ehYMsu4TJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save:  \n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "    ...\n",
        "\n",
        "}, PATH)\n",
        "\n",
        "# load:\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# or\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeRFhvkI4eKn",
        "colab_type": "text"
      },
      "source": [
        "# save/load entire model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CjAdF8Y4f0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(model, PATH)\n",
        "  \n",
        "# load\n",
        "# model class must be defined somewhere\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8ryz8J04lhS",
        "colab_type": "text"
      },
      "source": [
        "# saving and loading model state_dict for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vxd22xB4oyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# load:\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7p9jakt54Ux",
        "colab_type": "text"
      },
      "source": [
        "# warmstarting model using parameters from a different model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67WgRCrx56hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save\n",
        "torch.save(modelA.state_dict(), PATH)\n",
        "  \n",
        "# load\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "modelB.load_state_dict(torch.load(PATH), strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbudFfEBNPV6",
        "colab_type": "text"
      },
      "source": [
        "# CNNEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ9Rdy9NNQvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNEncoder, self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, padding=0),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU()\n",
        "                               )\n",
        "    self.layer4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU()\n",
        "                               )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYGVjmKBOZS7",
        "colab_type": "text"
      },
      "source": [
        "# RelationNetwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPsfhtiEOa9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RelationNetwork(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(RelationNetwork, self).__init__()\n",
        "    self.layer1 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                               nn.BatchNorm2d(64, momentum=1, affine=True),\n",
        "                               nn.ReLU(),\n",
        "                               nn.MaxPool2d(2))\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.fc2 = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(x)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.sigmoid(self.fc2(out))\n",
        "    return out\n",
        "\n",
        "def weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "    if m.bias is not None:\n",
        "      m.bias.data.zero_()\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    m.weight.data.fill_(1)\n",
        "    m.bias.data.zero_()\n",
        "  elif classname.find('Linear') != -1:\n",
        "    n = m.weight.size(1)\n",
        "    m.weight.data.normal_(0, 0.01)\n",
        "    m.bias.data = torch.ones(m.bias.data.size())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMA1ea-efEc7",
        "colab_type": "text"
      },
      "source": [
        "# FullyContextualEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3_fJ0e4fIui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyContextualEmbedding(nn.Module):\n",
        "  def __init__(self, feat_dim):\n",
        "    super(FullyContextualEmbedding, self).__init__()\n",
        "    self.lstmcell = nn.LSTMCell(feat_dim+2, feat_dim)\n",
        "    self.softmax = nn.Softmax()\n",
        "    self.c_0 = Variable(torch.zeros(1, feat_dim))\n",
        "    self.feat_dim = feat_dim\n",
        "    \n",
        "  def forward(self, f, G):\n",
        "    h = f\n",
        "    c = self.c_0.expand_as(f)\n",
        "    G_T = G.transpose(0, 1)\n",
        "    K = G.size(0)\n",
        "    for k in range(K):\n",
        "      logit_a = h.mm(G_T)\n",
        "      a = self.softmax(logit_a)\n",
        "      r = a.mm(G)\n",
        "      x = torch.cat((f, r), 1)\n",
        "      \n",
        "      h, c = self.lstmcell(x, (h, c))\n",
        "      h = h + f\n",
        "    return h\n",
        "  \n",
        "  def cuda(self):\n",
        "    super(FullyContextualEmbedding, self).cuda()\n",
        "    self.c_0 = self.c_0.cuda()\n",
        "    return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLb4aCtvPe_W",
        "colab_type": "text"
      },
      "source": [
        "# init seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnwA6sbyPghF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_seed(opt):\n",
        "  \"\"\"\n",
        "  Disable cudnn to maximize reproducibility\n",
        "  \"\"\"\n",
        "  torch.cuda.cudnn_enabled = False\n",
        "  np.random.seed(opt.manual_seed)\n",
        "  torch.manual_seed(opt.manual_seed)\n",
        "  torch.cuda.manual_seed(opt.manual_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lF4okkW4oeI",
        "colab_type": "text"
      },
      "source": [
        "# show sixteen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJidVTOy4r38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_sixteen(images, titles=0):\n",
        "    f, axarr = plt.subplots(4, 4, figsize=(15, 15), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
        "    for idx, ax in enumerate(f.axes):\n",
        "        ax.imshow(images[idx])\n",
        "        ax.axis(\"off\")\n",
        "        if titles: ax.set_title(titles[idx])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fovBEoJX4wpE",
        "colab_type": "text"
      },
      "source": [
        "# show enhanced and original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWHnMf2R4zs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_enhanced_and_original(enhanced_img, titles=0):\n",
        "    f, axarr = plt.subplots(1, 2, figsize=(10, 6))\n",
        "    axarr[0].imshow(img)\n",
        "    axarr[0].axis(\"off\")\n",
        "    axarr[0].set_title(titles[0])\n",
        "    axarr[1].imshow(enhanced_img)\n",
        "    axarr[1].axis(\"off\")\n",
        "    axarr[1].set_title(titles[1])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqCLNydU45A3",
        "colab_type": "text"
      },
      "source": [
        "# show three magnitudes and original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWFnUfKV4846",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_three_magnitutes_and_original(images, titles=0):\n",
        "    f, axarr = plt.subplots(1, 4, figsize=(20, 10))\n",
        "    for idx, ax in enumerate(axarr):\n",
        "        if idx==0: ax.imshow(img)\n",
        "        else: ax.imshow(images[idx-1])\n",
        "        ax.axis(\"off\")\n",
        "        if titles: ax.set_title(titles[idx])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPTa7Ugq5Eys",
        "colab_type": "text"
      },
      "source": [
        "# rotate with fill"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7JmdfQK5GPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_with_fill(img, magnitude):\n",
        "    im2 = img.convert(\"RGBA\")\n",
        "    rot = im2.rotate(magnitude)\n",
        "    fff = Image.new(\"RGBA\", rot.size, (128,) * 4)\n",
        "    out = Image.composite(rot, fff, rot)\n",
        "    return out.convert(img.mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxtjy5Xd5X2A",
        "colab_type": "text"
      },
      "source": [
        "# auto augment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSVgmGCp5aJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class ImageNetPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n",
        "        Example:\n",
        "        >>> policy = ImageNetPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     ImageNetPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n",
        "            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n",
        "            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n",
        "            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n",
        "            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment ImageNet Policy\"\n",
        "\n",
        "\n",
        "class CIFAR10Policy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n",
        "        Example:\n",
        "        >>> policy = CIFAR10Policy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     CIFAR10Policy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n",
        "            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n",
        "            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n",
        "            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n",
        "            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n",
        "            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n",
        "            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n",
        "            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n",
        "            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n",
        "            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n",
        "\n",
        "            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n",
        "            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment CIFAR10 Policy\"\n",
        "\n",
        "\n",
        "class SVHNPolicy(object):\n",
        "    \"\"\" Randomly choose one of the best 25 Sub-policies on SVHN.\n",
        "        Example:\n",
        "        >>> policy = SVHNPolicy()\n",
        "        >>> transformed = policy(image)\n",
        "        Example as a PyTorch Transform:\n",
        "        >>> transform=transforms.Compose([\n",
        "        >>>     transforms.Resize(256),\n",
        "        >>>     SVHNPolicy(),\n",
        "        >>>     transforms.ToTensor()])\n",
        "    \"\"\"\n",
        "    def __init__(self, fillcolor=(128, 128, 128)):\n",
        "        self.policies = [\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.2, \"invert\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 8, 0.7, \"invert\", 5, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 5, 0.6, \"solarize\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 3, 0.6, \"equalize\", 3, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 1, 0.9, \"rotate\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.8, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 8, 0.4, \"invert\", 5, fillcolor),\n",
        "            SubPolicy(0.9, \"shearY\", 5, 0.2, \"solarize\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 6, 0.8, \"autocontrast\", 1, fillcolor),\n",
        "            SubPolicy(0.6, \"equalize\", 3, 0.9, \"rotate\", 3, fillcolor),\n",
        "\n",
        "            SubPolicy(0.9, \"shearX\", 4, 0.3, \"solarize\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 8, 0.7, \"invert\", 4, fillcolor),\n",
        "            SubPolicy(0.9, \"equalize\", 5, 0.6, \"translateY\", 6, fillcolor),\n",
        "            SubPolicy(0.9, \"invert\", 4, 0.6, \"equalize\", 7, fillcolor),\n",
        "            SubPolicy(0.3, \"contrast\", 3, 0.8, \"rotate\", 4, fillcolor),\n",
        "\n",
        "            SubPolicy(0.8, \"invert\", 5, 0.0, \"translateY\", 2, fillcolor),\n",
        "            SubPolicy(0.7, \"shearY\", 6, 0.4, \"solarize\", 8, fillcolor),\n",
        "            SubPolicy(0.6, \"invert\", 4, 0.8, \"rotate\", 4, fillcolor),\n",
        "            SubPolicy(0.3, \"shearY\", 7, 0.9, \"translateX\", 3, fillcolor),\n",
        "            SubPolicy(0.1, \"shearX\", 6, 0.6, \"invert\", 5, fillcolor),\n",
        "\n",
        "            SubPolicy(0.7, \"solarize\", 2, 0.6, \"translateY\", 7, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 4, 0.8, \"invert\", 8, fillcolor),\n",
        "            SubPolicy(0.7, \"shearX\", 9, 0.8, \"translateY\", 3, fillcolor),\n",
        "            SubPolicy(0.8, \"shearY\", 5, 0.7, \"autocontrast\", 3, fillcolor),\n",
        "            SubPolicy(0.7, \"shearX\", 2, 0.1, \"invert\", 5, fillcolor)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        policy_idx = random.randint(0, len(self.policies) - 1)\n",
        "        return self.policies[policy_idx](img)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"AutoAugment SVHN Policy\"\n",
        "\n",
        "\n",
        "class SubPolicy(object):\n",
        "    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n",
        "        ranges = {\n",
        "            \"shearX\": np.linspace(0, 0.3, 10),\n",
        "            \"shearY\": np.linspace(0, 0.3, 10),\n",
        "            \"translateX\": np.linspace(0, 150 / 331, 10),\n",
        "            \"translateY\": np.linspace(0, 150 / 331, 10),\n",
        "            \"rotate\": np.linspace(0, 30, 10),\n",
        "            \"color\": np.linspace(0.0, 0.9, 10),\n",
        "            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n",
        "            \"solarize\": np.linspace(256, 0, 10),\n",
        "            \"contrast\": np.linspace(0.0, 0.9, 10),\n",
        "            \"sharpness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"brightness\": np.linspace(0.0, 0.9, 10),\n",
        "            \"autocontrast\": [0] * 10,\n",
        "            \"equalize\": [0] * 10,\n",
        "            \"invert\": [0] * 10\n",
        "        }\n",
        "\n",
        "        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n",
        "        def rotate_with_fill(img, magnitude):\n",
        "            rot = img.convert(\"RGBA\").rotate(magnitude)\n",
        "            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n",
        "\n",
        "        func = {\n",
        "            \"shearX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"shearY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n",
        "                Image.BICUBIC, fillcolor=fillcolor),\n",
        "            \"translateX\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n",
        "                fillcolor=fillcolor),\n",
        "            \"translateY\": lambda img, magnitude: img.transform(\n",
        "                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n",
        "                fillcolor=fillcolor),\n",
        "            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n",
        "            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n",
        "            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n",
        "            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n",
        "            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n",
        "            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n",
        "                1 + magnitude * random.choice([-1, 1])),\n",
        "            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n",
        "            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n",
        "            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n",
        "        }\n",
        "\n",
        "        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n",
        "        #     operation1, ranges[operation1][magnitude_idx1],\n",
        "        #     operation2, ranges[operation2][magnitude_idx2])\n",
        "        self.p1 = p1\n",
        "        self.operation1 = func[operation1]\n",
        "        self.magnitude1 = ranges[operation1][magnitude_idx1]\n",
        "        self.p2 = p2\n",
        "        self.operation2 = func[operation2]\n",
        "        self.magnitude2 = ranges[operation2][magnitude_idx2]\n",
        "\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n",
        "        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sphccv_KKZ89",
        "colab_type": "text"
      },
      "source": [
        "# tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwAmofm9KbpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with test_summary_writer.as_default():\n",
        "          summary.scalar('loss', test_loss, step=self.globaliter)\n",
        "          summary.scalar('accuracy', accuracy, step=self.globaliter)\n",
        "\n",
        "with train_summary_writer.as_default():\n",
        "          tf.summary.scalar('loss', loss.item(), step=globaliter)\n",
        "\n",
        "@tf.function\n",
        "def my_func(step, loss):\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar(\"loss\", loss.item(), step)\n",
        "    tf.summary.histogram(\"weights\", w)\n",
        "    tf.summary.image('input', x_image, 3)\n",
        "# call this function during training\n",
        "my_func(globaliter, loss)\n",
        "\n",
        "    \n",
        "%tensorboard --logdir logs/tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dYUs9pwy96L",
        "colab_type": "text"
      },
      "source": [
        "# check runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR13GeLmy_ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RXoucIy4msV",
        "colab_type": "text"
      },
      "source": [
        "# weights_init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRMdFXtz4oub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "def weights_init_normal(m):\n",
        "# 2\n",
        "def weights_init_xavier(m):\n",
        "# 3\n",
        "def weights_init_kaiming(m):\n",
        "# 4\n",
        "def weights_init_orthogonal(m):\n",
        "  classname = m.__class__.__name__\n",
        "  # print(classname)\n",
        "  if classname.find(\"Conv\") != -1:\n",
        "    # 1\n",
        "    init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    # 2\n",
        "    init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    # 3\n",
        "    init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    # 4\n",
        "    init.orthogonal(m.weight.data, gain=1)\n",
        "  elif classname.find(\"Linear\") != -1:\n",
        "    # 1\n",
        "    init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    # 2\n",
        "    init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    # 3\n",
        "    init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    # 4\n",
        "    init.orthogonal(m.weight.data, gain=1)\n",
        "  elif classname.find(\"BatchNorm2d\") != -1:\n",
        "    init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def init_layer(L):\n",
        "  print(\"init_layer\")\n",
        "  # Initialization using fan-in\n",
        "  if isinstance(L, nn.Conv2d):\n",
        "      n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
        "      L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
        "  elif isinstance(L, nn.BatchNorm2d):\n",
        "      L.weight.data.fill_(1)\n",
        "      L.bias.data.fill_(0)\n",
        "\n",
        "def _initialization(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "        \n",
        "for m in self.modules():\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "    init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "  elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "    init.constant_(m.weight, 1)\n",
        "    init.constant_(m.bias, 0)\n",
        "\n",
        "def init_weights(net, init_type='normal'):\n",
        "  print('initialization method [%s]' % init_type)\n",
        "  if init_type == 'normal':\n",
        "    net.apply(weights_init_normal)\n",
        "  elif init_type == 'xavier':\n",
        "    net.apply(weights_init_xavier)\n",
        "  elif init_type == 'kaiming':\n",
        "    net.apply(weights_init_kaiming)\n",
        "  elif init_type == 'orthogonal':\n",
        "    net.apply(weights_init_orthogonal)\n",
        "  else:\n",
        "    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVxLEFhX5Wj9",
        "colab_type": "text"
      },
      "source": [
        "# ResidualBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZQErX4O5YtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, in_features):\n",
        "  # 2\n",
        "  def __init__(self, in_features, norm=\"in\"):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    \n",
        "    conv_block = [\n",
        "        nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n",
        "        nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True)\n",
        "    ]\n",
        "    \n",
        "    # 2\n",
        "    norm_layer = AdaptiveInstanceNorm2d if norm == \"adain\" else nn.InstanceNorm2d\n",
        "    \n",
        "    conv_block = [\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(in_features, in_features, 3),\n",
        "        norm_layer(in_features),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(in_features, in_features, 3),\n",
        "        norm_layer(in_features),        \n",
        "    ]\n",
        "    \n",
        "    self.conv_block = nn.Sequential(*conv_block)\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    return x + self.conv_block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYLtkimU7CDM",
        "colab_type": "text"
      },
      "source": [
        "# GeneratorResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaPSgvMB7EB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorResNet(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, out_channels=3, dim=64, n_upsample=2, shared_block=None):\n",
        "  # 2\n",
        "  def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n",
        "    super(GeneratorResNet, self).__init__()\n",
        "    \n",
        "    # 1/2\n",
        "    self.shared_block = shared_block\n",
        "    \n",
        "    layers = []\n",
        "    dim = dim * 2 ** n_upsample\n",
        "    \n",
        "    channels, img_size, _ = img_shape\n",
        "    \n",
        "    # initial convolutional block\n",
        "    model = [\n",
        "        nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n",
        "        nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(inplace=True)\n",
        "    ]\n",
        "    \n",
        "    # downsampling\n",
        "    curr_dim = 64\n",
        "    for _ in range(2):\n",
        "      model += [\n",
        "          nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n",
        "          nn.InstanceNorm2d(curr_dim=2, affine=True, track_running_stats=True),\n",
        "          nn.ReLU(inplace=True)\n",
        "      ]\n",
        "      curr_dim *= 2\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(res_blocks):\n",
        "      model += [ResidualBlock(curr_dim)]\n",
        "    \n",
        "    # upsampling\n",
        "    for _ in range(2):\n",
        "      mode += [\n",
        "          nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n",
        "          # 1\n",
        "          nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n",
        "          # 2\n",
        "          nn.InstanceNorm2d(curr_dim // 2)\n",
        "          # 1\n",
        "          nn.ReLU(inplace=True)\n",
        "          # 2\n",
        "          nn.LeakyReLU(0.2, inplace=True),          \n",
        "      ]\n",
        "      curr_dim = curr_dim // 2\n",
        "    \n",
        "    # output layer\n",
        "    model += [# 1/2 \n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3),\n",
        "        nn.Tanh()]\n",
        "    \n",
        "    self.model = nn.Sequential(*model)\n",
        "  \n",
        "  # 1\n",
        "  def forward(self, x, c):\n",
        "    c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "    c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "    x = torch.cat((x, c), 1)\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdexZ6Q8Sb2",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXaQs_q8UPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n",
        "  # 2\n",
        "  def __init__(self, input_shape):\n",
        "  # 3\n",
        "  def __init__(self):\n",
        "  # 4\n",
        "  def __init__(self, in_channels=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # 1\n",
        "    channels, img_size, _ = img_shape\n",
        "    \n",
        "    # 2\n",
        "    channels, height, width = input_shape\n",
        "    \n",
        "    # calculate output of image discriminator (PatchGAN)\n",
        "    # 1\n",
        "    self.output_shape = (1, height // 2 ** 3, width // 2 ** 3)\n",
        "    # 2\n",
        "    self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "    \n",
        "    # 1\n",
        "    def discriminator_block(in_filters, out_filters):\n",
        "      \"\"\"\n",
        "      Returns downsampling layers of each discriminator block\n",
        "      \"\"\"\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n",
        "      return layers\n",
        "    \n",
        "    # 2\n",
        "    def discriminator_block(in_filters, out_filters, normalization=True):\n",
        "      \"\"\"\n",
        "      Returns downsampling layers of each discriminator block\n",
        "      \"\"\"\n",
        "      # 1\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "      # 2\n",
        "      layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True),\n",
        "               nn.Dropout2d(0.25)]\n",
        "      if normalization:\n",
        "        # 1\n",
        "        layers.append(nn.InstanceNorm2d(out_filters))\n",
        "        # 2\n",
        "        layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    # 3\n",
        "    def discriminator_block(in_filters, out_filters, first_block=False):\n",
        "      layers = []\n",
        "      layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n",
        "      if not first_block:\n",
        "        layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n",
        "      layers.append(nn.BatchNorm2d(out_filters))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    \n",
        "    \n",
        "    \n",
        "    # 1\n",
        "    layers = discriminator_block(channels, 64)\n",
        "    curr_dim = 64\n",
        "    for _ in range(n_strided - 1):\n",
        "      layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n",
        "      curr_dim *= 2\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    # output 1: PatchGAN\n",
        "    self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n",
        "    \n",
        "    # output 2: class prediction\n",
        "    kernel_size = img_size // 2 ** n_strided\n",
        "    self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n",
        "  \n",
        "    # 2\n",
        "    self.model = nn.Sequential(\n",
        "        # 1\n",
        "        *discriminator_block(channels, 64, normalization=False),\n",
        "        # 2\n",
        "        *discriminator_block(channels * 2, 64, normalization=False),\n",
        "        *discriminator_block(64, 128),\n",
        "        # 1\n",
        "        *discriminator_block(128, 128),\n",
        "        # 2\n",
        "        *discriminator_block(128, 256),\n",
        "        *discriminator_block(256, 512),\n",
        "        \n",
        "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "        # 1\n",
        "        nn.Conv2d(256, 1, 4, padding=1)\n",
        "        # 2\n",
        "        nn.Conv2d(256, 1, 4)\n",
        "        # 3\n",
        "        nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
        "        # 4\n",
        "        nn.Conv2d(512, 1, 3, padding=1)\n",
        "    )\n",
        "    \n",
        "    # 3\n",
        "    self.model = nn.Sequential(\n",
        "        # 1\n",
        "        nn.Linear(opt.img_size ** 2, 512),\n",
        "        # 2\n",
        "        nn.Linear(int(np.prod(img_shape)), 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(256, 1),\n",
        "        # 1/2\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "  \n",
        "    # 4\n",
        "    \n",
        "    layers = []\n",
        "    in_filters = in_channels\n",
        "    for i, out_filters in enumerate([64, 128, 256, 512]):\n",
        "      layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n",
        "      in_filters = out_filters\n",
        "    \n",
        "    layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    # 5\n",
        "    self.model = nn.Sequential(\n",
        "        *discriminator_block(channels, 16, normalization=False),\n",
        "        *discriminator_block(16, 32),\n",
        "        *discriminator_block(32, 64),\n",
        "        *discriminator_block(64, 128),\n",
        "        \n",
        "    )\n",
        "    \n",
        "    # the height and width of downsampled image\n",
        "    ds_size = img_size // 2 ** 4\n",
        "    self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), \n",
        "                                   # 1/2\n",
        "                                   nn.Sigmoid())\n",
        "    # 1/2\n",
        "    self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, n_classes), nn.Softmax())\n",
        "    \n",
        "    # 6\n",
        "    # upsampling\n",
        "    self.down = nn.Sequential(nn.Conv2d(channels, 64, 3, 2, 1), nn.ReLU())\n",
        "    # fully connected layers\n",
        "    self.down_size = img_size // 2\n",
        "    down_dim = 64 * (img_size // 2) ** 2\n",
        "    \n",
        "    self.embedding = nn.Linear(down_dim, 32)\n",
        "    \n",
        "    self.fc = nn.Sequential(\n",
        "        nn.BatchNorm1d(32, 0.8),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(32, down_dim),\n",
        "        nn.BatchNorm1d(down_dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    \n",
        "    # upsampling\n",
        "    self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, channels, 3, 1, 1))\n",
        "  \n",
        "  # 1\n",
        "  def forward(self, img):\n",
        "    feature_repr = self.model(img)\n",
        "    out_adv = self.out1(feature_repr)\n",
        "    out_cls = self.out2(feature_repr)\n",
        "    return out_adv, out_cls.view(out_cls.size(0), -1)\n",
        "  \n",
        "  # 2\n",
        "  def forward(self, img_A, img_B):\n",
        "    # concatenate image and condition image by channels to produce input\n",
        "    img_input = torch.cat((img_A, img_B), 1)\n",
        "    return self.model((img_input))\n",
        "  \n",
        "  # 3\n",
        "  def forward(self, img):\n",
        "    img_flat = img.view(img.shape[0], -1)\n",
        "    validity = self.model(img_flat)\n",
        "    return validity\n",
        "  \n",
        "  # 4\n",
        "  def forward(self, img):\n",
        "    out = self.model(img)\n",
        "    out = out.view(out.shape[0], -1)\n",
        "    validity = self.adv_layer(out)\n",
        "    return validity\n",
        "  \n",
        "  # 5\n",
        "  def forward(self, img):\n",
        "    out = self.down(img)\n",
        "    embedding = self.embedding(out.view(out.size(0), -1))\n",
        "    out = self.fc(embedding)\n",
        "    out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n",
        "    return out, embedding\n",
        "  \n",
        "  # 6\n",
        "  def forward(self, img):\n",
        "    out = self.model(img)\n",
        "    out = out.view(out.shape[0], -1)\n",
        "    validity = self.adv_layer(out)\n",
        "    label = self.aux_layer(out)\n",
        "    return validity, label\n",
        "  \n",
        "  # 7\n",
        "  def forward(self, img):\n",
        "    return self.model(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYBel5CXCUXy",
        "colab_type": "text"
      },
      "source": [
        "# FeatureExtractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCy3zz-xCWV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    vgg19_model = models.vgg19(pretrained=True)\n",
        "    self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
        "  \n",
        "  def forward(self, img):\n",
        "    return self.feature_extractor(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxPkHqv7XQgQ",
        "colab_type": "text"
      },
      "source": [
        "# UNetDown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKe1lYfLXSUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# U-NET\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "  def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "    super(UNetDown, self).__init__()\n",
        "    # 1\n",
        "    layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n",
        "    # 2\n",
        "    layers = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n",
        "    if normalize:\n",
        "      # 1\n",
        "      layers.append(nn.BatchNorm2d(out_size, 0.8))\n",
        "      # 2\n",
        "      layers.append(nn.InstanceNorm2d(out_size))\n",
        "      # 3\n",
        "      layers.append(nn.InstanceNorm2d(out_size, affine=True))\n",
        "    if dropout:\n",
        "      layers.append(nn.Dropout(dropout))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiyWZtUZXTuo",
        "colab_type": "text"
      },
      "source": [
        "# UNetUp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yliKqdwXYyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNetUp(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, in_size, out_size, dropout=0.0):\n",
        "  # 2\n",
        "  def __init__(self, in_size, out_size):\n",
        "    super(UNetUp, self).__init__()\n",
        "    # 1\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_size, 0.8),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    layers = [# 1\n",
        "              nn.ConvTranspose2d(in_size, out_size, 4, 2, 1), \n",
        "              # 2\n",
        "              nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False), \n",
        "              # 1\n",
        "              nn.InstanceNorm2d(out_size), \n",
        "              # 2\n",
        "              nn.BatchNorm2d(out_size, 0.8),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if dropout:\n",
        "      layers.append(nn.Dropout(dropout))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "  def forward(self, x, skip_input):\n",
        "    x = self.model(x)\n",
        "    x = torch.cat((x, skip_input), 1)\n",
        "    return x\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3JM77fQakca",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO7TlbaqamRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "class Encoder(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, latent_dim, input_shape):\n",
        "  # 2\n",
        "  def __init__(self, latent_dim):\n",
        "\n",
        "    super(Encoder, self).__init__()\n",
        "    # 1\n",
        "    resnet18_model = resnet18(pretrained=False)\n",
        "    self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
        "    self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
        "    # output is mu and log(var) for reparameterization rick used in VAEs\n",
        "    self.fc_mu = nn.Linear(256, latent_dim)\n",
        "    self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "    # 2\n",
        "    resnet152_model = resnet152(pretrained=True)\n",
        "    self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-1])\n",
        "    self.final = nn.Sequential(\n",
        "        nn.Linear(resnet152_model.in_features, latent_dim),\n",
        "        nn.BatchNorm1d(latent_dim, momentum=0.01)\n",
        "    )\n",
        "    \n",
        "  # 1\n",
        "  def forward(self, img):\n",
        "    out = self.feature_extractor(img)\n",
        "    out = self.pooling(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    mu = self.fc_mu(out)\n",
        "    logvar = self.fc_logvar(out)\n",
        "    return mu, logvar\n",
        "  # 2\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = self.feature_extractor(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return self.final(x)\n",
        "# 2\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n",
        "    super(Encoder, self).__init__()\n",
        "  \n",
        "    self.content_encoder = ContentEncoder(in_channels, dim, n_residual, n_downsample)\n",
        "    self.style_encoder = StyleEncoder(in_channels, dim, n_downsample, style_dim)\n",
        "    \n",
        "  def forward(self, img):\n",
        "    content_code = self.content_encoder(x)\n",
        "    style_code = self.style_encoder(x)\n",
        "    return content_code, style_code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idCkpHgwehUt",
        "colab_type": "text"
      },
      "source": [
        "# GeneratorUNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKDJAWy4eigL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorUNet(nn.Module):\n",
        "  # 1\n",
        "  def __init__(self, latent_dim, img_shape):\n",
        "  # 2\n",
        "  def __init__(self, channels=3):\n",
        "  # 3\n",
        "  def __init__(self, in_channels=3, out_channels=3):\n",
        "    super(GeneratorUNet, self).__init__()\n",
        "    channels, self.h, self.w = img_shape\n",
        "    \n",
        "    # 1\n",
        "    self.fc = nn.Linear(latent_dim, self.h * self.w)\n",
        "    \n",
        "    # 1\n",
        "    self.down1 = UNetDown(channels + 1, 64, normalize=False)\n",
        "    # 2\n",
        "    self.down1 = UNetDown(channels, 64, normalize=False)\n",
        "    self.down2 = UNetDown(64, 128)\n",
        "    # 1\n",
        "    self.down3 = UNetDown(128, 256)\n",
        "    # 2\n",
        "    self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n",
        "    # 3\n",
        "    self.down3 = UNetDown(128, 256, dropout=0.5)\n",
        "    # 1\n",
        "    self.down4 = UNetDown(256, 512)\n",
        "    self.down5 = UNetDown(512, 512)\n",
        "    self.down6 = UNetDown(512, 512)\n",
        "    self.down7 = UNetDown(512, 512, normalize=False)\n",
        "    # 2\n",
        "    self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "    self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "    self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "    self.down7 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
        "    # 1\n",
        "    self.up1 = UNetUp(512, 512)\n",
        "    self.up2 = UNetUp(1024, 512)\n",
        "    self.up3 = UNetUp(1024, 512)\n",
        "    # 2\n",
        "    self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "    self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "    self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
        "    # 1/2\n",
        "    self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
        "    # 1\n",
        "    self.up4 = UNetUp(1024, 256)\n",
        "    self.up5 = UNetUp(512, 128)\n",
        "    self.up6 = UNetUp(256, 64)\n",
        "    # 2\n",
        "    self.up5 = UNetUp(1024, 256)\n",
        "    self.up6 = UNetUp(512, 128)\n",
        "    self.up7 = UNetUp(256, 64)\n",
        "  \n",
        "    # 1\n",
        "    self.final = nn.Sequential(\n",
        "        nn.Upsample(scale_factor),\n",
        "        nn.Conv2d(128, channels, 3, stride=1, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    channels, _, _ = input_shape\n",
        "    \n",
        "    self.down1 = UNetDown(channels, 64, normalize=False)\n",
        "    self.down2 = UNetDown(64, 128)\n",
        "    self.down3 = UNetDown(128, 256, dropout=0.5)\n",
        "    self.down4 = UNetDown(256, 512, dropout=0.5)\n",
        "    self.down5 = UNetDown(512, 512, dropout=0.5)\n",
        "    # 1\n",
        "    self.down6 = UNetDown(512, 512, dropout=0.5, normalize=False)\n",
        "    # 2\n",
        "    self.down6 = UNetDown(512, 512, dropout=0.5)\n",
        "    \n",
        "    self.up1 = UNetUp(512, 512, dropout=0.5)\n",
        "    self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
        "    self.up3 = UNetUp(1024, 256)\n",
        "    self.up4 = UNetUp(512, 128)\n",
        "    self.up5 = UNetUp(256, 64)\n",
        "    \n",
        "    # 2\n",
        "    self.final = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "        # 1\n",
        "        nn.Conv2d(128, channels, 4, padding=1),\n",
        "        # 2\n",
        "        nn.Conv2d(128, out_channels, 4, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "    # 3\n",
        "    self.final = nn.Sequential(\n",
        "        nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  \n",
        "  def forward(self, x, c):\n",
        "    # 1\n",
        "    # propagate noise through fc layer and reshape to img shape\n",
        "    z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n",
        "    d1 = self.down1(torch.cat((x, z), 1))\n",
        "    d2 = self.down2(d1)\n",
        "    d3 = self.down3(d2)\n",
        "    d4 = self.down4(d3)\n",
        "    d5 = self.down5(d4)\n",
        "    d6 = self.down6(d5)\n",
        "    d7 = self.down7(d6)\n",
        "    u1 = self.up1(d7, d6)\n",
        "    u2 = self.up2(u1, d5)\n",
        "    u3 = self.up3(u2, u4)\n",
        "    u4 = self.up4(u3, d3)\n",
        "    u5 = self.up5(u4, d2)\n",
        "    u6 = self.up6(u5, d1)\n",
        "    return self.final(u6)\n",
        "  \n",
        "    # 2\n",
        "    # U-Net generator with skip connections from encoder to decoder\n",
        "    d1 = self.down1(x)\n",
        "    d2 = self.down2(d1)\n",
        "    d2 = torch.cat((d2, x_lr), 1)\n",
        "    d3 = self.down3(d2)\n",
        "    d4 = self.down4(d3)\n",
        "    d5 = self.down5(d4)\n",
        "    d6 = self.down6(d5)\n",
        "    u1 = self.up1(d6, d5)\n",
        "    u2 = self.up2(u1, d4)\n",
        "    u3 = self.up3(u2, d3)\n",
        "    u4 = self.up4(u3, d2)\n",
        "    u5 = self.up5(u4, d1)\n",
        "    return self.final(u5)\n",
        "  \n",
        "    # 3\n",
        "    # U-Net generator with skip connections from encoder to decoder\n",
        "    d1 = self.down1(x)\n",
        "    d2 = self.down2(d1)\n",
        "    # d2 = torch.cat((d2, x_lr), 1)\n",
        "    d3 = self.down3(d2)\n",
        "    d4 = self.down4(d3)\n",
        "    d5 = self.down5(d4)\n",
        "    d6 = self.down6(d5)\n",
        "    d7 = self.down7(d6)\n",
        "    d8 = self.down8(d7)\n",
        "    u1 = self.up1(d8, d7)\n",
        "    u2 = self.up2(u1, d6)\n",
        "    u3 = self.up3(u2, d5)\n",
        "    u4 = self.up4(u3, d4)\n",
        "    u5 = self.up5(u4, d3)\n",
        "    u6 = self.up6(u5, d2)\n",
        "    u7 = self.up7(u6, d1)\n",
        "    return self.final(u7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IpmmdbVocon",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpeAqZbCod4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    # 1\n",
        "    def block(in_feat, out_feat, normalize=True):\n",
        "      layers = [nn.Linear(in_feat, out_feat)]\n",
        "      if normalize:\n",
        "        layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "      return layers\n",
        "    \n",
        "    self.model = nn.Sequential(\n",
        "        # 1\n",
        "        *block(latent_dim, 128, normalize=False),\n",
        "        # 2\n",
        "        *block(latent_dim + n_classes, 128, normalize=False),\n",
        "        *block(128, 256),\n",
        "        *block(256, 512),\n",
        "        *block(512, 1024),\n",
        "        nn.Linear(1024, int(np.prod(img_shape))),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "    # 2\n",
        "    \n",
        "    # 1/2\n",
        "    self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
        "    \n",
        "    self.init_size = img_size // 4\n",
        "    self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
        "    \n",
        "    self.conv_blocks = nn.Sequential(\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128, 0.8),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64, 0.8),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "    \n",
        "  # 1  \n",
        "  def forward(self, z):\n",
        "    img = self.model(z)\n",
        "    img = img.view(img.shape[0], *img_shape)\n",
        "    return img\n",
        "  # 2\n",
        "  def forward(self, noise):\n",
        "    out = self.l1(noise)\n",
        "    out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "    img = self.conv_blocks(out)\n",
        "    return img\n",
        "  # 3\n",
        "  def forward(self, noise, labels):\n",
        "    gen_input = torch.mul(self.label_emb(labels), noise)\n",
        "    out = self.l1(gen_input)\n",
        "    out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "    img = self.conv_blocks(out)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_eMay8z9psP",
        "colab_type": "text"
      },
      "source": [
        "# GAN optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVVIiyse9rDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer, example, Adadelta, Adagrad, SparseAdam, Adamax, ASGD, LBFGS, RMSprop, Rprop, SGD\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6H5GGu_TdC",
        "colab_type": "text"
      },
      "source": [
        "# gradient penalty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c50eBQI1_WcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "def compute_gradient_penalty(D, X):\n",
        "# 2\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "  \"\"\"\n",
        "  calculates the gradient penalty loss for \n",
        "  # 1\n",
        "  DRAGAN\n",
        "  # 2\n",
        "  WGAN GP\n",
        "  \"\"\"\n",
        "  # 1\n",
        "  # random weight term for interpolation\n",
        "  alpha = Tensor(np.random.random(size=X.shape))\n",
        "  # 2\n",
        "  # random weight term for interpolation between real and fake samples\n",
        "  alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "  \n",
        "  # 1\n",
        "  interpolates = alpha * X + ((1 - alpha) * (X + 0.5 * X.std() * torch.rand(X.size())))\n",
        "  # 2\n",
        "  # get random interpolation between real and fake samples\n",
        "  interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "  \n",
        "  interpolates = Variable(interpoates, requires_grad=True)\n",
        "  \n",
        "  d_interpolates = D(interpolates)\n",
        "  fake = Variable(Tensor(X.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "  \n",
        "  # get gradient with respect to interpolates\n",
        "  gradients = autograd.grad(\n",
        "      outputs=d_interpolates,\n",
        "      inputs=interpolates,\n",
        "      grad_outputs=fake,\n",
        "      create_graph=True,\n",
        "      retain_graph=True,\n",
        "      only_inputs=True,\n",
        "  )[0]\n",
        "  # 1/2\n",
        "  gradients = gradients.view(gradients.size(0), -1)\n",
        "  \n",
        "  # 1\n",
        "  gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "  # 2\n",
        "  gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "  return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTirMZB1Fb4",
        "colab_type": "text"
      },
      "source": [
        "# initialize generator and discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqJ5Dith1JBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "# initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# 2\n",
        "generator = GeneratorResNet().to(device)\n",
        "discriminator = Discriminator(input_shape=(channels, *hr_shape)).to(device)\n",
        "feature_extractor = FeatureExtractor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAAb_J6Z1QGK",
        "colab_type": "text"
      },
      "source": [
        "# GAN optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1I1MC261SOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_G = Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKTnnyQG3RGG",
        "colab_type": "text"
      },
      "source": [
        "# GAN hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D18rbYk3TVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GAN hyperparameters\n",
        "\n",
        "epoch = 0 # epoch to start training from\n",
        "num_epochs = 10\n",
        "dataset_name = \"apple2orange\"\n",
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999 # adam: deccay of first order momentum of gradient\n",
        "decay_epoch = 100 # epoch from which to start lr decay\n",
        "latent_dim = 100 # dimensionality of the latent space\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "img_size = 28 # size of each image dimension\n",
        "# n_classes = 10 # number of classes for dataset\n",
        "channels = 3 # number of image channels\n",
        "# n_critic = 5 # number of training steps for discriminator per iter\n",
        "# clip_value = 0.01 # lower and upper clip value for disc. weights\n",
        "sample_interval = 100 # interval between image samples\n",
        "dim = 64 # number of filters in first encoder layer\n",
        "checkpoint_interval = -1 # interval between saving model checkpoints\n",
        "n_downsample = 2 # number downsampling layers in encoder\n",
        "\n",
        "img_shape = (channels, img_size, img_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFK1vLcFgbB",
        "colab_type": "text"
      },
      "source": [
        "# boundary seeking loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts23pTIXFjY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def boundary_seeking_loss(y_pred, y_true):\n",
        "  return 0.5 * torch.mean((torch.log(y_pred) - torch.log(1 - y_pred)) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCejXsNQKT1O",
        "colab_type": "text"
      },
      "source": [
        "# GAN train generator, discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ3OV7MGKfYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile('./model.tar'):\n",
        "  checkpoint = torch.load('model.tar')\n",
        "  generator.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "  discriminator.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "  optimizer_G.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "  optimizer_D.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "prev_time = time.time()\n",
        "iters = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs, _) in enumerate(mnist_loader):\n",
        "    \n",
        "    # adversarial ground truths\n",
        "    valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
        "    \n",
        "    # configure input\n",
        "    real_imgs = Variable(imgs.type(Tensor))\n",
        "    \n",
        "    # TRAIN GENERATOR\n",
        "    \n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    # sample noise as generator input\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "    \n",
        "    # generate a batch of images\n",
        "    gen_imgs = generator(z)\n",
        "    gen_labels = Variable(LongTensor(np.random.randint(0, n_classes, imgs.shape[0])))\n",
        "    \n",
        "    \n",
        "    # 1\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = boundary_seeking_loss(discriminator(gen_imgs), valid)\n",
        "    \n",
        "    # 2\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "    \n",
        "    # 3\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = pixelwise_loss(recon_images, gen_imgs.detach() + lambda_pt * pullaway_loss(img_embeddings))\n",
        "    \n",
        "    # 4\n",
        "    # 1\n",
        "    \n",
        "    real_pred = discriminator(real_imgs).detach()\n",
        "    fake_pred = discriminator(gen_imgs)\n",
        "    \n",
        "    if rel_avg_gan:\n",
        "      g_loss = adversarial_loss(fake_pred - real_pred.mean(0, keepdim=True), valid)\n",
        "    else:\n",
        "      g_loss = adversarial_loss(fake_pred - real_pred, valid)\n",
        "    \n",
        "    # 2\n",
        "    # loss measures generator's ability to fool the discriminator\n",
        "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "    \n",
        "    # 5\n",
        "    validity, pred_label = discriminator(gen_imgs)\n",
        "    g_loss = 0.5 * adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels)\n",
        "    \n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    writer.add_images('pred_fake', pred_fake, iters)\n",
        "    writer.add_scalar('loss_GAN', loss_GAN, iters)\n",
        "    \n",
        "    # TRAIN DISCRIMINATOR\n",
        "    \n",
        "    optimizer_D.zero_grad()\n",
        "    \n",
        "    # 1\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_loss = discriminator_loss(discriminator(real_imgs), valid)\n",
        "    fake_loss = discriminator_loss(discriminator(gen_imgs.detach()), fake)\n",
        "    \n",
        "    # 2\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "    fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "    \n",
        "    d_loss = (real_loss + fake_loss) / 2\n",
        "    \n",
        "    # 3\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    real_recon, _ = discriminator(real_imgs)\n",
        "    fake_recon, _ = discriminator(gen_imgs.detach())\n",
        "    \n",
        "    d_loss_real = pixelwise_loss(real_recon, real_imgs)\n",
        "    d_loss_fake = pixelwise_loss(fake_recon, gen_imgs.detach())\n",
        "    \n",
        "    d_loss = d_loss_real\n",
        "    if (margin - d_loss_fake.data).item() > 0:\n",
        "      d_loss += margin - d_loss_fake\n",
        "    \n",
        "    # 4\n",
        "    # measure discriminator's ability to classify real from generated samples\n",
        "    if rel_avg_gan:\n",
        "      real_loss = adversarial_loss(real_pred - fake_pred.mean(0, keepdim=True), valid)\n",
        "      fake_loss = adversarial_loss(real_pred - real_pred.mean(0, keepdim=True), fake)\n",
        "    else:\n",
        "      real_loss = adversarial_loss(real_pred - fake_pred, valid)\n",
        "      fake_loss = adversarial_loss(fake_pred - real_pred, fake)\n",
        "    \n",
        "    # 5\n",
        "    # loss for real images\n",
        "    real_pred, real_aux = discriminator(real_imgs)\n",
        "    d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
        "    \n",
        "    # loss for fake images\n",
        "    fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
        "    d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
        "    \n",
        "    d_loss = (real_loss + fake_loss) / 2  \n",
        "\n",
        "    \n",
        "    # call this function during training\n",
        "    # my_func(i, d_loss, g_loss)\n",
        "    \n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "    \n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    batches_left = num_epochs * len(dataloader) - batches_done\n",
        "    \n",
        "    time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "    prev_time = time.time()\n",
        "    \n",
        "    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] ETA: %s\"% (epoch, num_epochs, i, len(mnist_loader),\n",
        "                                                                    d_loss.item(), g_loss.item(),  time_left,\n",
        "                                                                   ))\n",
        "    \n",
        "    if batches_done % sample_interval == 0:\n",
        "      # 1\n",
        "      sample_images(n_row=10, batches_done=batches_done)\n",
        "      # 2\n",
        "      save_image(gen_imgs.data[:25], \"./%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "    \n",
        "    # Check how the generator is doing by saving G's output on fixed_noise\n",
        "    # if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(mnist_loader)-1)):\n",
        "    #     with torch.no_grad():\n",
        "    #         fake = generator(fixed_noise).detach().cpu()\n",
        "    #    img_list.append(make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "    \n",
        "    torch.save({\n",
        "        'modelA_state_dict': generator.state_dict(),\n",
        "        'modelB_state_dict': discriminator.state_dict(),\n",
        "        'optimizerA_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizerB_state_dict': optimizer_D.state_dict()\n",
        "    }, 'model.tar')\n",
        "    \n",
        "    iters += 1\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17VepsDJCcvo",
        "colab_type": "text"
      },
      "source": [
        "# apply weight init normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs815ElmCfCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQli9V0sEC6z",
        "colab_type": "text"
      },
      "source": [
        "# GAN loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV38GACzEEp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reconstruction loss of AE\n",
        "pixelwise_loss = nn.MSELoss()\n",
        "\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# minimize MSE instead of BCE\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "\n",
        "adversarial_loss = torch.nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "auxiliary_loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnIaETSeJbMj",
        "colab_type": "text"
      },
      "source": [
        "# GAN tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGUz-gbLJcjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def my_func(step, d_loss, g_loss):\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar(\"d_loss\", d_loss.item(), step)\n",
        "    tf.summary.scalar(\"g_loss\", d_loss.item(), step)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE0iNEHALs8N",
        "colab_type": "text"
      },
      "source": [
        "# LambdaLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbGmDSBZLunS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LambdaLR:\n",
        "  def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "    assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "    self.n_epochs = n_epochs\n",
        "    self.offset = offset\n",
        "    self.decay_start_epoch = decay_start_epoch\n",
        "  \n",
        "  def step(self, epoch):\n",
        "    return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgWAhMX-OYZ7",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVF7lqVcOZfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    layers = []\n",
        "    dim = dim * 2 ** n_upsample\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(n_residual):\n",
        "      layers += [ResidualBlock(dim, norm=\"adain\")]\n",
        "    \n",
        "    # upsampling\n",
        "    for _ in range(n_upsample):\n",
        "      layers += [\n",
        "          nn.Upsample(scale_factor=2),\n",
        "          nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n",
        "          LayerNorm(dim // 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "      ]\n",
        "      dim = dim // 2\n",
        "    \n",
        "    # output layer\n",
        "    layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    # initiate mlp (predicts AdaIN parameters)\n",
        "    num_adain_params = self.get_num_adain_params()\n",
        "    self.mlp = MLP(style_dim, num_adain_params)\n",
        "  \n",
        "  def get_num_adain_params(self):\n",
        "    \"\"\"\n",
        "    return the number of AdaIN parameters needed by the model\n",
        "    \"\"\"\n",
        "    num_adain_params = 0\n",
        "    for m in self.modules():\n",
        "      if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
        "        num_adain_params += 2 * m.num_features\n",
        "    return num_adain_params\n",
        "  \n",
        "  def assign_adain_params(self, adain_params):\n",
        "    \"\"\"\n",
        "    assign the adain_params to the AdaIN layers in model\n",
        "    \"\"\"\n",
        "    for m in self.modules():\n",
        "      if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
        "        # extract mean and std predictions\n",
        "        mean = adain_params[:, : m.num_features]\n",
        "        std = adain_params[:, m.num_features : 2 * m.num_features]\n",
        "        # update bias and weight\n",
        "        m.bias = mean.contiguous().view(-1)\n",
        "        m.weight = std.contiguous().view(-1)\n",
        "        # move pointer\n",
        "        if adain_params.size(1) > 2 * m.num_features:\n",
        "          adain_params = adain_params[:, 2 * m.num_features :]\n",
        "  \n",
        "  def forward(self, content_code, style_code):\n",
        "    # update AdaIN parameters by MLP prediction based off style code\n",
        "    self.assign_adain_params(self.mlp(style_code))\n",
        "    img = self.model(content_code)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICR09H6OUk3i",
        "colab_type": "text"
      },
      "source": [
        "# ContentEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czH8HpS-Umr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContentEncoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2):\n",
        "    super(ContentEncoder, self).__init__()\n",
        "    \n",
        "    # initial convolution block\n",
        "    layers = [\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(in_channels, dim, 7),\n",
        "        nn.InstanceNorm2d(dim),\n",
        "        nn.ReLU(inplace=True),\n",
        "    ]\n",
        "    \n",
        "    # downsampling\n",
        "    for _ in range(n_downsample):\n",
        "      layers += [\n",
        "          nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n",
        "          nn.InstanceNorm2d(dim * 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "      ]\n",
        "      dim *= 2\n",
        "    \n",
        "    # residual blocks\n",
        "    for _ in range(n_residual):\n",
        "      layers += [ResidualBlock(dim, norm=\"in\")]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QbQ4o0UnIN",
        "colab_type": "text"
      },
      "source": [
        "# StyleEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BDm3TRIUo0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "  def __init__(self, in_channels=3, dim=64, n_downsample=2, style_dim=0):\n",
        "    super(StyleEncoder, self).__init__()\n",
        "    \n",
        "    # initial conv block\n",
        "    layers = [nn.ReflectionPad2d(3), nn.Conv2d(in_channels, dim, 7), nn.ReLU(inplace=True)]\n",
        "    \n",
        "    # downsampling\n",
        "    for _ in range(2):\n",
        "      layers += [nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n",
        "      \n",
        "    # downsampling with constant depth\n",
        "    for _ in range(n_downsample - 2):\n",
        "      layers += [nn.Conv2d(dim, dim, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n",
        "    \n",
        "    layers += [nn.AdaptiveAvgPool2d(1), nn.Conv2d(dim, style_dim, 1, 1, 0)]\n",
        "    \n",
        "    self.model = nn.Sequential(*layers)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTuDmVOCV8Ge",
        "colab_type": "text"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Z47vtwV92x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MLP (predicts AdaIn parameters)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, dim=256, n_blk=3, activ=\"relu\"):\n",
        "    super(MLP, self).__init__()\n",
        "    layers = [nn.Linear(input_dim, dim), nn.ReLU(inplace=True)]\n",
        "    for _ in range(n_blk - 2):\n",
        "      layers += [nn.Linear(dim, dim), nn.ReLU(inplace=True)]\n",
        "    layers += [nn.Linear(dim, output_dim)]\n",
        "    self.model = nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x.view(x.size(0), -1))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYvYtY2uZHkk",
        "colab_type": "text"
      },
      "source": [
        "# AdaptiveInstanceNorm2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUVN4rR8ZJWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveInstanceNorm2d(nn.Module):\n",
        "  def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "    super(AdaptiveInstanceNorm2d, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    # weight and bias are dynamically assigned\n",
        "    self.weight = None\n",
        "    self.bias = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    assert (\n",
        "      self.weight is not None and self.bias is not None\n",
        "    ), \"please assing weight and bias before calling AdaIN\"\n",
        "    b, c, h, w = x.size()\n",
        "    running_mean = self.running_mean.repeat(b)\n",
        "    running_var = self.running_var.repeat(b)\n",
        "    \n",
        "    # apply instance norm\n",
        "    x_reshaped = x.contiguous().view(1, b * c, h, w)\n",
        "    \n",
        "    out = F.batch_norm(\n",
        "        x_reshaped, running_mean, running_var, self.weight, self.bias, True, self.momentum, self.eps\n",
        "    )\n",
        "    \n",
        "    return out.view(b, c, h, w)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + \"(\" + str(self.num_features) + \")\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOPQeM-aTTl",
        "colab_type": "text"
      },
      "source": [
        "# LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2iaeJhNaUSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, num_features, eps=1e-5, affine=True):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.affine = affine\n",
        "    self.eps = eps\n",
        "    \n",
        "    if self.affine:\n",
        "      self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
        "      self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    shape = [-1] + [1] * (x.dim() - 1)\n",
        "    mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
        "    std = x.view(x.size(0), -1).std(1).view(*shape)\n",
        "    x = (x - mean) / (std + self.eps)\n",
        "    \n",
        "    if self.affine:\n",
        "      shape = [1, -1] + [1] * (x.dim() - 2)\n",
        "      x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF4qeEYpRJND",
        "colab_type": "text"
      },
      "source": [
        "# plot train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv2yN71hDq6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmE0oadGzb1",
        "colab_type": "text"
      },
      "source": [
        "# visualize GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq47CXJnG02e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxfpLiJxHiFO",
        "colab_type": "text"
      },
      "source": [
        "# real images vs fake images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTH6ZWcJHlBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdXsiGPh_jqm",
        "colab_type": "text"
      },
      "source": [
        "# torchvision repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de2dh8mn_lHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip uninstall -y torchvision\n",
        "\n",
        "pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO-UuXFINFkL",
        "colab_type": "text"
      },
      "source": [
        "# pick one image and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7TJIdSwNH6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[0]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  prediction = model([img.to(device)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rCEJq8tNNoz",
        "colab_type": "text"
      },
      "source": [
        "# train, evaluate using helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9MJpxtXNRUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let us train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # train for one epoch, printing every 10 iterations\n",
        "  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "  # update the learning rate\n",
        "  lr_scheduler.step()\n",
        "  # evaluate on the test dataset\n",
        "  evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQEvlqeyNfNg",
        "colab_type": "text"
      },
      "source": [
        "# transforms helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hse50tWKNgsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "  transforms = []\n",
        "  transforms.append(T.ToTensor())\n",
        "  if train:\n",
        "    transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "  return T.Compose(transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qecglzH4NmRY",
        "colab_type": "text"
      },
      "source": [
        "# instance segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HTymQPRNnij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_instance_segmentation_model(num_classes):\n",
        "  # load an instance segmentation model pre-trained on COCO\n",
        "  model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "  \n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  \n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  \n",
        "  # now get the number of input features for the mask classifier\n",
        "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "  hidden_layer = 256\n",
        "  \n",
        "  # and replace the mask predictor with a new one\n",
        "  model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "  \n",
        "  return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOXl-iCENq5w",
        "colab_type": "text"
      },
      "source": [
        "# modify model to add a different backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYG_5aKONtig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return only the features\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "\n",
        "# FasterRCNN needs to know the number of output channels in a backbone.\n",
        "# for mobilenet_v2, it is 1280 so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let us make the RPN generate 5 x 3 anchors per spatial location, with 5 different sizes and 3 different\n",
        "# aspect ratios.\n",
        "# we have a Tuple[Tuple[int]] because each faeture map could potentially have different sizes and aspect ratios\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let us define what are the feature maps that we will use to perform the region of interest cropping, as well\n",
        "# as the size of the crop after rescaling.\n",
        "# if our backbone returns a Tensor, featmap_names is expected to be [0].\n",
        "# more generally, the backbone should return an OrderedDict[Tensor], and in featmap_names we can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0], output_size=7, sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TevY7yyNNxsq",
        "colab_type": "text"
      },
      "source": [
        "# finetune from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "806JUHc2Nzq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch.nn as nn\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has num_classes which is user-defined\n",
        "num_classes = 2 # 1 class (person) + background\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbJ1CbuZN7AK",
        "colab_type": "text"
      },
      "source": [
        "# object detection custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ZrK-yWN9Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class PennFudanDataset(object):\n",
        "  def __init__(self, root, transforms):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # we load all image files, sorting them to ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    # load images and masks\n",
        "    img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "    \n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # we have not converted the mask to RGB, because each color corresponds to a different instance with\n",
        "    # 0 being background\n",
        "    \n",
        "    mask = Image.open(mask_path)\n",
        "    \n",
        "    # convert the PIL Image into a numpy array\n",
        "    mask = np.array(mask)\n",
        "    \n",
        "    # instances are encoded as different colors\n",
        "    obj_ids = np.unique(mask)\n",
        "    \n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "    \n",
        "    # split the color-encoded mask into a set of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "    \n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    for i in range(num_objs):\n",
        "      pos = np.where(masks[i])\n",
        "      xmin = np.min(pos[1])\n",
        "      xmax = np.max(pos[1])\n",
        "      ymin = np.min(pos[0])\n",
        "      ymax = np.max(pos[0])\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      \n",
        "    # convert everything into a torch.Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    \n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "    \n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    \n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "    \n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    \n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "      \n",
        "    return img, target\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDLssEzX_bFs",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ528Xvy_cmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9IkEdP_tGsl",
        "colab_type": "text"
      },
      "source": [
        "# ngrok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6P00dNRctrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6vL2ys3sKJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = '../gdrive/My Drive/runs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N2NogNJsW5_",
        "colab_type": "code",
        "outputId": "d3b71c37-9148-4327-deac-8086f1ac71e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://e8279752.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml9gdD6288z3",
        "colab_type": "text"
      },
      "source": [
        "# kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akJ8fvsfGVdn",
        "colab_type": "code",
        "outputId": "b23301ca-f1dd-42c4-b43c-49dea2bc01c6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ee6c5dc-439d-4c58-9dd1-61a1a6001fe3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1ee6c5dc-439d-4c58-9dd1-61a1a6001fe3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDLXrqcNGlU7",
        "colab_type": "code",
        "outputId": "7e936d29-63c8-4bad-e8b3-37d918a719c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 64 Jun  1 20:46 kaggle.json\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QITWh1N5GoqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KixiXyagGy7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPZaJ7oOGuyh",
        "colab_type": "code",
        "outputId": "a00d767e-c171-42ed-a1bf-e4976b13ca5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# List available datasets.\n",
        "!kaggle datasets list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                                             title                                     size  lastUpdated          downloadCount  \r\n",
            "--------------------------------------------------------------  ---------------------------------------  -----  -------------------  -------------  \r\n",
            "stackoverflow/stack-overflow-2018-developer-survey              Stack Overflow 2018 Developer Survey      20MB  2018-05-15 16:59:54            376  \r\n",
            "ruslankl/mice-protein-expression                                Mice Protein Expression                  987KB  2018-05-06 15:09:39            311  \r\n",
            "jameslko/gun-violence-data                                      Gun Violence Data                         34MB  2018-04-15 06:18:09           5438  \r\n",
            "jessicali9530/honey-production                                  Honey Production In The USA (1998-2012)   80KB  2018-04-09 23:31:19           2384  \r\n",
            "donorschoose/io                                                 Data Science For Good: DonorsChoose.org    1GB  2018-05-14 19:26:30           3858  \r\n",
            "datafiniti/grammar-and-online-product-reviews                   Grammar and Online Product Reviews         9MB  2018-02-15 17:20:27            572  \r\n",
            "datagov/usa-names                                               USA Name Data                            335MB  2018-04-20 22:14:20              0  \r\n",
            "nycopendata/new-york                                            NYC Open Data                            189GB  2018-04-19 17:22:13              0  \r\n",
            "ardamavi/sign-language-digits-dataset                           Sign Language Digits Dataset               8MB  2017-12-24 16:08:56           1344  \r\n",
            "unitednations/global-commodity-trade-statistics                 Global Commodity Trade Statistics        121MB  2017-11-14 23:55:11           2248  \r\n",
            "moltean/fruits                                                  Fruits 360 dataset                       198MB  2018-05-26 08:05:33           3717  \r\n",
            "goldenoakresearch/us-acs-mortgage-equity-loans-rent-statistics  Insightful & Vast USA Statistics          10MB  2018-05-19 19:35:52           1510  \r\n",
            "ainslie/usda-wasde-monthly-corn-soybean-projections             Corn & Soybean Prices 2008-2017          487KB  2018-05-10 00:14:43            714  \r\n",
            "paultimothymooney/blood-cells                                   Blood Cell Images                        113MB  2018-04-21 21:06:13           1785  \r\n",
            "xvivancos/star-wars-movie-scripts                               Star Wars Movie Scripts                  268KB  2018-05-07 13:57:15            593  \r\n",
            "skihikingkevin/pubg-match-deaths                                PUBG Match Deaths and Statistics           4GB  2018-01-12 15:22:04           2674  \r\n",
            "sayangoswami/reddit-memes-dataset                               Reddit Memes Dataset                     634MB  2018-04-16 19:13:18            396  \r\n",
            "google/tinyquickdraw                                            QuickDraw Sketches                        11GB  2018-04-18 19:38:04            107  \r\n",
            "joaoevangelista/wta-matches-and-rankings                        WTA Matches and Rankings                  20MB  2017-11-15 03:01:12            705  \r\n",
            "vikasg/russian-troll-tweets                                     Russian Troll Tweets                      21MB  2018-02-15 00:49:04           1153  \r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyJ4Rb5HMwH",
        "colab_type": "code",
        "outputId": "046e79f7-b040-45f1-9aa2-1b07dd3b3289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Copy the stackoverflow data set locally.\n",
        "!kaggle datasets download -d stackoverflow/stack-overflow-2018-developer-survey"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stack-overflow-2018-developer-survey.zip: Downloaded 20MB of 20MB to /content/.kaggle/datasets/stackoverflow/stack-overflow-2018-developer-survey\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FgC4GlvHZQ1",
        "colab_type": "code",
        "outputId": "d9c67cc2-7087-4e3c-d16a-a4be4d81965f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!head ~/.kaggle/datasets/stackoverflow/stack-overflow-2018-developer-survey/survey_results_public.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Respondent,Hobby,OpenSource,Country,Student,Employment,FormalEducation,UndergradMajor,CompanySize,DevType,YearsCoding,YearsCodingProf,JobSatisfaction,CareerSatisfaction,HopeFiveYears,JobSearchStatus,LastNewJob,AssessJob1,AssessJob2,AssessJob3,AssessJob4,AssessJob5,AssessJob6,AssessJob7,AssessJob8,AssessJob9,AssessJob10,AssessBenefits1,AssessBenefits2,AssessBenefits3,AssessBenefits4,AssessBenefits5,AssessBenefits6,AssessBenefits7,AssessBenefits8,AssessBenefits9,AssessBenefits10,AssessBenefits11,JobContactPriorities1,JobContactPriorities2,JobContactPriorities3,JobContactPriorities4,JobContactPriorities5,JobEmailPriorities1,JobEmailPriorities2,JobEmailPriorities3,JobEmailPriorities4,JobEmailPriorities5,JobEmailPriorities6,JobEmailPriorities7,UpdateCV,Currency,Salary,SalaryType,ConvertedSalary,CurrencySymbol,CommunicationTools,TimeFullyProductive,EducationTypes,SelfTaughtTypes,TimeAfterBootcamp,HackathonReasons,AgreeDisagree1,AgreeDisagree2,AgreeDisagree3,LanguageWorkedWith,LanguageDesireNextYear,DatabaseWorkedWith,DatabaseDesireNextYear,PlatformWorkedWith,PlatformDesireNextYear,FrameworkWorkedWith,FrameworkDesireNextYear,IDE,OperatingSystem,NumberMonitors,Methodology,VersionControl,CheckInCode,AdBlocker,AdBlockerDisable,AdBlockerReasons,AdsAgreeDisagree1,AdsAgreeDisagree2,AdsAgreeDisagree3,AdsActions,AdsPriorities1,AdsPriorities2,AdsPriorities3,AdsPriorities4,AdsPriorities5,AdsPriorities6,AdsPriorities7,AIDangerous,AIInteresting,AIResponsible,AIFuture,EthicsChoice,EthicsReport,EthicsResponsible,EthicalImplications,StackOverflowRecommend,StackOverflowVisit,StackOverflowHasAccount,StackOverflowParticipate,StackOverflowJobs,StackOverflowDevStory,StackOverflowJobsRecommend,StackOverflowConsiderMember,HypotheticalTools1,HypotheticalTools2,HypotheticalTools3,HypotheticalTools4,HypotheticalTools5,WakeTime,HoursComputer,HoursOutside,SkipMeals,ErgonomicDevices,Exercise,Gender,SexualOrientation,EducationParents,RaceEthnicity,Age,Dependents,MilitaryUS,SurveyTooLong,SurveyEasy\r\n",
            "1,Yes,No,Kenya,No,Employed part-time,\"Bachelors degree (BA, BS, B.Eng., etc.)\",Mathematics or statistics,20 to 99 employees,Full-stack developer,3-5 years,3-5 years,Extremely satisfied,Extremely satisfied,Working as a founder or co-founder of my own company,\"Im not actively looking, but I am open to new opportunities\",Less than a year ago,10,7,8,1,2,5,3,4,9,6,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,3,1,4,2,5,5,6,7,2,1,4,3,My job status or other personal status changed,NA,NA,Monthly,NA,KES,Slack,One to three months,\"Taught yourself a new language, framework, or tool without taking a formal course;Participated in a hackathon\",\"The official documentation and/or standards for the technology;A book or e-book from OReilly, Apress, or a similar publisher;Questions & answers on Stack Overflow;Online developer communities other than Stack Overflow (ex. forums, listservs, IRC channels, etc.)\",NA,To build my professional network,Strongly agree,Strongly agree,Neither Agree nor Disagree,JavaScript;Python;HTML;CSS,JavaScript;Python;HTML;CSS,\"Redis;SQL Server;MySQL;PostgreSQL;Amazon RDS/Aurora;Microsoft Azure (Tables, CosmosDB, SQL, etc)\",\"Redis;SQL Server;MySQL;PostgreSQL;Amazon RDS/Aurora;Microsoft Azure (Tables, CosmosDB, SQL, etc)\",AWS;Azure;Linux;Firebase,AWS;Azure;Linux;Firebase,Django;React,Django;React,Komodo;Vim;Visual Studio Code,Linux-based,1,Agile;Scrum,Git,Multiple times per day,Yes,No,NA,Strongly agree,Strongly agree,Strongly agree,Saw an online advertisement and then researched it (without clicking on the ad);Stopped going to a website because of their advertising,1,5,4,7,2,6,3,\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",Algorithms making important decisions,The developers or the people creating the AI,I'm excited about the possibilities more than worried about the dangers.,No,\"Yes, and publicly\",Upper management at the company/organization,Yes,10 (Very Likely),Multiple times per day,Yes,I have never participated in Q&A on Stack Overflow,\"No, I knew that Stack Overflow had a jobs board but have never used or visited it\",Yes,NA,Yes,Extremely interested,Extremely interested,Extremely interested,Extremely interested,Extremely interested,Between 5:00 - 6:00 AM,9 - 12 hours,1 - 2 hours,Never,Standing desk,3 - 4 times per week,Male,Straight or heterosexual,\"Bachelors degree (BA, BS, B.Eng., etc.)\",Black or of African descent,25 - 34 years old,Yes,NA,The survey was an appropriate length,Very easy\r\n",
            "3,Yes,Yes,United Kingdom,No,Employed full-time,\"Bachelors degree (BA, BS, B.Eng., etc.)\",\"A natural science (ex. biology, chemistry, physics)\",\"10,000 or more employees\",Database administrator;DevOps specialist;Full-stack developer;System administrator,30 or more years,18-20 years,Moderately dissatisfied,Neither satisfied nor dissatisfied,Working in a different or more specialized technical role than the one I'm in now,I am actively looking for a job,More than 4 years ago,1,7,10,8,2,5,4,3,6,9,1,5,3,7,10,4,11,9,6,2,8,3,1,5,2,4,1,3,4,5,2,6,7,I saw an employers advertisement,British pounds sterling (),51000,Yearly,70841,GBP,\"Confluence;Office / productivity suite (Microsoft Office, Google Suite, etc.);Slack;Other wiki tool (Github, Google Sites, proprietary software, etc.)\",One to three months,\"Taught yourself a new language, framework, or tool without taking a formal course;Contributed to open source software\",The official documentation and/or standards for the technology;Questions & answers on Stack Overflow,NA,NA,Agree,Agree,Neither Agree nor Disagree,JavaScript;Python;Bash/Shell,Go;Python,Redis;PostgreSQL;Memcached,PostgreSQL,Linux,Linux,Django,React,IPython / Jupyter;Sublime Text;Vim,Linux-based,2,NA,Git;Subversion,A few times per week,Yes,Yes,The website I was visiting asked me to disable it,Somewhat agree,Neither agree nor disagree,Neither agree nor disagree,NA,3,5,1,4,6,7,2,Increasing automation of jobs,Increasing automation of jobs,The developers or the people creating the AI,I'm excited about the possibilities more than worried about the dangers.,Depends on what it is,Depends on what it is,Upper management at the company/organization,Yes,10 (Very Likely),A few times per month or weekly,Yes,A few times per month or weekly,Yes,\"No, I have one but it's out of date\",7,Yes,A little bit interested,A little bit interested,A little bit interested,A little bit interested,A little bit interested,Between 6:01 - 7:00 AM,5 - 8 hours,30 - 59 minutes,Never,Ergonomic keyboard or mouse,Daily or almost every day,Male,Straight or heterosexual,\"Bachelors degree (BA, BS, B.Eng., etc.)\",White or of European descent,35 - 44 years old,Yes,NA,The survey was an appropriate length,Somewhat easy\r\n",
            "4,Yes,Yes,United States,No,Employed full-time,Associate degree,\"Computer science, computer engineering, or software engineering\",20 to 99 employees,Engineering manager;Full-stack developer,24-26 years,6-8 years,Moderately satisfied,Moderately satisfied,Working as a founder or co-founder of my own company,\"Im not actively looking, but I am open to new opportunities\",Less than a year ago,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA\r\n",
            "5,No,No,United States,No,Employed full-time,\"Bachelors degree (BA, BS, B.Eng., etc.)\",\"Computer science, computer engineering, or software engineering\",100 to 499 employees,Full-stack developer,18-20 years,12-14 years,Neither satisfied nor dissatisfied,Slightly dissatisfied,Working as a founder or co-founder of my own company,\"Im not actively looking, but I am open to new opportunities\",Less than a year ago,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,A recruiter contacted me,U.S. dollars ($),NA,NA,NA,NA,NA,Three to six months,\"Completed an industry certification program (e.g. MCPD);Taught yourself a new language, framework, or tool without taking a formal course\",\"The official documentation and/or standards for the technology;A book or e-book from OReilly, Apress, or a similar publisher;Questions & answers on Stack Overflow;The technologys online help system\",NA,NA,Disagree,Disagree,Strongly disagree,C#;JavaScript;SQL;TypeScript;HTML;CSS;Bash/Shell,C#;JavaScript;SQL;TypeScript;HTML;CSS;Bash/Shell,\"SQL Server;Microsoft Azure (Tables, CosmosDB, SQL, etc)\",\"SQL Server;Microsoft Azure (Tables, CosmosDB, SQL, etc)\",Azure,Azure,NA,Angular;.NET Core;React,Visual Studio;Visual Studio Code,Windows,2,Agile;Kanban;Scrum,Git,Multiple times per day,Yes,Yes,The ad-blocking software was causing display issues on a website,Neither agree nor disagree,Somewhat agree,Somewhat agree,Stopped going to a website because of their advertising,NA,NA,NA,NA,NA,NA,NA,\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",A governmental or other regulatory body,\"I don't care about it, or I haven't thought about it.\",No,\"Yes, but only within the company\",Upper management at the company/organization,Yes,10 (Very Likely),A few times per week,Yes,A few times per month or weekly,Yes,\"No, I have one but it's out of date\",8,Yes,Somewhat interested,Somewhat interested,Somewhat interested,Somewhat interested,Somewhat interested,Between 6:01 - 7:00 AM,9 - 12 hours,Less than 30 minutes,3 - 4 times per week,NA,I don't typically exercise,Male,Straight or heterosexual,Some college/university study without earning a degree,White or of European descent,35 - 44 years old,No,No,The survey was an appropriate length,Somewhat easy\r\n",
            "7,Yes,No,South Africa,\"Yes, part-time\",Employed full-time,Some college/university study without earning a degree,\"Computer science, computer engineering, or software engineering\",\"10,000 or more employees\",Data or business analyst;Desktop or enterprise applications developer;Game or graphics developer;QA or test developer;Student,6-8 years,0-2 years,Slightly satisfied,Moderately satisfied,Working in a different or more specialized technical role than the one I'm in now,\"Im not actively looking, but I am open to new opportunities\",Between 1 and 2 years ago,8,5,7,1,2,6,4,3,10,9,1,10,2,4,8,3,11,7,5,9,6,2,1,4,5,3,7,3,6,2,1,4,5,My job status or other personal status changed,South African rands (R),260000,Yearly,21426,ZAR,\"Office / productivity suite (Microsoft Office, Google Suite, etc.)\",Three to six months,\"Taken a part-time in-person course in programming or software development;Received on-the-job training in software development;Taught yourself a new language, framework, or tool without taking a formal course\",\"The official documentation and/or standards for the technology;A book or e-book from OReilly, Apress, or a similar publisher;Questions & answers on Stack Overflow\",NA,NA,Strongly agree,Agree,Strongly disagree,C;C++;Java;Matlab;R;SQL;Bash/Shell,Assembly;C;C++;Matlab;SQL;Bash/Shell,SQL Server;PostgreSQL;Oracle;IBM Db2,PostgreSQL;Oracle;IBM Db2,Arduino;Windows Desktop or Server,Arduino;Windows Desktop or Server,NA,NA,Notepad++;Visual Studio;Visual Studio Code,Windows,2,Evidence-based software engineering;Formal standard such as ISO 9001 or IEEE 12207 (aka waterfall methodologies),Zip file back-ups,Weekly or a few times per month,No,NA,NA,Somewhat agree,Somewhat agree,Somewhat disagree,Clicked on an online advertisement;Saw an online advertisement and then researched it (without clicking on the ad),2,3,4,6,1,7,5,Algorithms making important decisions,Algorithms making important decisions,The developers or the people creating the AI,I'm excited about the possibilities more than worried about the dangers.,No,\"Yes, but only within the company\",Upper management at the company/organization,Yes,10 (Very Likely),Daily or almost daily,Yes,Less than once per month or monthly,\"No, I knew that Stack Overflow had a jobs board but have never used or visited it\",\"No, I know what it is but I don't have one\",NA,Yes,Extremely interested,Extremely interested,Extremely interested,Extremely interested,Extremely interested,Before 5:00 AM,Over 12 hours,1 - 2 hours,Never,NA,3 - 4 times per week,Male,Straight or heterosexual,Some college/university study without earning a degree,White or of European descent,18 - 24 years old,Yes,NA,The survey was an appropriate length,Somewhat easy\r\n",
            "8,Yes,No,United Kingdom,No,Employed full-time,\"Bachelors degree (BA, BS, B.Eng., etc.)\",\"Computer science, computer engineering, or software engineering\",10 to 19 employees,Back-end developer;Database administrator;Front-end developer;Full-stack developer,6-8 years,3-5 years,Moderately satisfied,Slightly satisfied,Working in a different or more specialized technical role than the one I'm in now,I am actively looking for a job,Between 2 and 4 years ago,8,5,4,9,1,3,6,2,10,7,1,3,4,10,9,2,6,5,11,7,8,4,2,5,1,3,2,6,7,3,1,5,4,I did not receive an expected change in compensation,British pounds sterling (),30000,NA,41671,GBP,\"Confluence;Jira;Office / productivity suite (Microsoft Office, Google Suite, etc.);Other chat system (IRC, proprietary software, etc.)\",Less than a month,\"Received on-the-job training in software development;Taught yourself a new language, framework, or tool without taking a formal course;Participated in online coding competitions (e.g. HackerRank, CodeChef, TopCoder)\",\"The official documentation and/or standards for the technology;Online developer communities other than Stack Overflow (ex. forums, listservs, IRC channels, etc.)\",NA,\"To improve my general technical skills or programming ability;To improve my knowledge of a specific programming language, framework, or other technology;Because I find it enjoyable\",Disagree,Neither Agree nor Disagree,Strongly disagree,Java;JavaScript;Python;TypeScript;HTML;CSS,C#;Go;Java;JavaScript;Python;SQL;TypeScript;HTML;CSS,MongoDB,PostgreSQL,Linux,Linux,Angular;Node.js,Node.js,IntelliJ;PyCharm;Visual Studio Code,Linux-based,2,Agile,Git,A few times per week,Yes,Yes,I wanted to support the website I was visiting by viewing their ads,Somewhat agree,Somewhat agree,Somewhat disagree,Saw an online advertisement and then researched it (without clicking on the ad);Stopped going to a website because of their advertising,1,3,4,2,7,5,6,Increasing automation of jobs,Algorithms making important decisions,A governmental or other regulatory body,I'm excited about the possibilities more than worried about the dangers.,Depends on what it is,Depends on what it is,Upper management at the company/organization,Unsure / I don't know,7,A few times per month or weekly,Yes,Less than once per month or monthly,Yes,\"No, I have one but it's out of date\",8,No,A little bit interested,Not at all interested,Very interested,Very interested,Extremely interested,Between 7:01 - 8:00 AM,9 - 12 hours,30 - 59 minutes,1 - 2 times per week,NA,1 - 2 times per week,Male,Straight or heterosexual,\"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\",White or of European descent,18 - 24 years old,No,NA,The survey was an appropriate length,Somewhat easy\r\n",
            "9,Yes,Yes,United States,No,Employed full-time,Some college/university study without earning a degree,\"Computer science, computer engineering, or software engineering\",\"10,000 or more employees\",Back-end developer;Front-end developer;Full-stack developer,9-11 years,0-2 years,Slightly satisfied,Moderately satisfied,Working as a founder or co-founder of my own company,\"Im not actively looking, but I am open to new opportunities\",Less than a year ago,5,3,9,4,1,8,2,7,10,6,1,3,2,9,11,4,8,6,7,10,5,3,1,5,4,2,1,5,3,4,2,6,7,My job status or other personal status changed,U.S. dollars ($),120000,Yearly,120000,USD,\"Confluence;Office / productivity suite (Microsoft Office, Google Suite, etc.);Stack Overflow Enterprise;Other chat system (IRC, proprietary software, etc.);Other wiki tool (Github, Google Sites, proprietary software, etc.)\",Six to nine months,\"Received on-the-job training in software development;Taught yourself a new language, framework, or tool without taking a formal course\",\"The official documentation and/or standards for the technology;Questions & answers on Stack Overflow;Online developer communities other than Stack Overflow (ex. forums, listservs, IRC channels, etc.)\",NA,NA,Disagree,Agree,Strongly disagree,JavaScript;HTML;CSS,C;Go;JavaScript;Python;HTML;CSS,MongoDB,NA,Linux,Linux,Node.js;React,React;TensorFlow,Atom;Visual Studio Code,MacOS,2,Agile;Scrum,Git,Multiple times per day,Yes,Yes,The ad-blocking software was causing display issues on a website,Somewhat disagree,Neither agree nor disagree,Somewhat disagree,Clicked on an online advertisement;Stopped going to a website because of their advertising,1,4,2,5,3,7,6,Algorithms making important decisions,\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",The developers or the people creating the AI,I'm excited about the possibilities more than worried about the dangers.,Depends on what it is,\"Yes, but only within the company\",Upper management at the company/organization,Yes,10 (Very Likely),Multiple times per day,Yes,I have never participated in Q&A on Stack Overflow,Yes,\"No, I have one but it's out of date\",7,No,Very interested,A little bit interested,Extremely interested,Very interested,Very interested,Between 9:01 - 10:00 AM,Over 12 hours,Less than 30 minutes,1 - 2 times per week,NA,I don't typically exercise,Male,Straight or heterosexual,\"Masters degree (MA, MS, M.Eng., MBA, etc.)\",White or of European descent,18 - 24 years old,No,No,The survey was an appropriate length,Somewhat easy\r\n",
            "10,Yes,Yes,Nigeria,No,Employed full-time,\"Bachelors degree (BA, BS, B.Eng., etc.)\",\"Computer science, computer engineering, or software engineering\",10 to 19 employees,Designer;Front-end developer;QA or test developer,0-2 years,3-5 years,Slightly satisfied,Moderately satisfied,Working as a founder or co-founder of my own company,\"Im not actively looking, but I am open to new opportunities\",Less than a year ago,6,5,4,2,7,8,10,1,9,3,1,3,5,7,6,2,11,9,4,10,8,1,3,2,4,5,2,6,1,3,7,5,4,I saw an employers advertisement,NA,NA,NA,NA,NA,\"Facebook;Google Hangouts/Chat;Office / productivity suite (Microsoft Office, Google Suite, etc.);Slack;Trello;Other wiki tool (Github, Google Sites, proprietary software, etc.)\",One to three months,\"Taken an online course in programming or software development (e.g. a MOOC);Participated in a full-time developer training program or bootcamp;Received on-the-job training in software development;Participated in online coding competitions (e.g. HackerRank, CodeChef, TopCoder);Contributed to open source software\",NA,Immediately after graduating,\"To improve my general technical skills or programming ability;To improve my knowledge of a specific programming language, framework, or other technology;To improve my ability to work on a team with other programmers;Because I find it enjoyable\",Strongly agree,Strongly disagree,Neither Agree nor Disagree,JavaScript;TypeScript;HTML;CSS,Matlab;SQL;Kotlin;Bash/Shell,\"MongoDB;MySQL;Microsoft Azure (Tables, CosmosDB, SQL, etc);Google Cloud Storage\",NA,Azure;Heroku,Amazon Echo;Android;Apple Watch or Apple TV;AWS;Google Cloud Platform/App Engine;Google Home;iOS;WordPress;Firebase,Angular;Node.js,.NET Core;Django,Atom;Notepad++;Sublime Text;Visual Studio Code,Windows,1,Agile;Extreme programming (XP);Scrum,Git,Multiple times per day,Yes,No,NA,Strongly agree,Neither agree nor disagree,Strongly disagree,Saw an online advertisement and then researched it (without clicking on the ad),NA,NA,NA,NA,NA,NA,NA,\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",\"Evolving definitions of \"\"fairness\"\" in algorithmic versus human decisions\",NA,I'm excited about the possibilities more than worried about the dangers.,Depends on what it is,\"Yes, but only within the company\",The developer who wrote it,Yes,10 (Very Likely),Daily or almost daily,Yes,A few times per month or weekly,\"No, I knew that Stack Overflow had a jobs board but have never used or visited it\",\"No, and I don't know what that is\",NA,Yes,Very interested,Very interested,Very interested,A little bit interested,Extremely interested,I do not have a set schedule,Over 12 hours,1 - 2 hours,Daily or almost every day,NA,1 - 2 times per week,Female,NA,Primary/elementary school,Black or of African descent,25 - 34 years old,No,NA,The survey was too long,Somewhat difficult\r\n",
            "11,Yes,Yes,United States,No,Employed full-time,Some college/university study without earning a degree,\"Fine arts or performing arts (ex. graphic design, music, studio art)\",100 to 499 employees,\"Back-end developer;C-suite executive (CEO, CTO, etc.);Data or business analyst;Database administrator;DevOps specialist;Engineering manager;Full-stack developer;System administrator\",30 or more years,21-23 years,Moderately satisfied,Moderately satisfied,Doing the same work,\"Im not actively looking, but I am open to new opportunities\",Between 2 and 4 years ago,6,3,7,4,1,5,10,8,9,2,1,3,2,9,11,5,8,4,10,7,6,5,1,2,3,4,3,7,2,4,1,6,5,My job status or other personal status changed,U.S. dollars ($),250000,Yearly,250000,USD,\"Confluence;HipChat;Jira;Office / productivity suite (Microsoft Office, Google Suite, etc.)\",Three to six months,\"Taken an online course in programming or software development (e.g. a MOOC);Taught yourself a new language, framework, or tool without taking a formal course;Participated in a hackathon;Contributed to open source software\",\"The official documentation and/or standards for the technology;A book or e-book from OReilly, Apress, or a similar publisher;A college/university computer science or software engineering book;Online developer communities other than Stack Overflow (ex. forums, listservs, IRC channels, etc.);Tapping your network of friends, family, and peers versed in the technology;The technologys online help system\",NA,Because I find it enjoyable,Strongly agree,Strongly disagree,Strongly disagree,Assembly;CoffeeScript;Erlang;Go;JavaScript;Lua;Python;Ruby;SQL;HTML;CSS;Bash/Shell,Erlang;Go;Python;Rust;SQL,Redis;PostgreSQL;Amazon DynamoDB;Apache HBase;Apache Hive;Amazon Redshift;Amazon RDS/Aurora;Elasticsearch,Redis;PostgreSQL;Amazon DynamoDB;Apache Hive;Amazon RDS/Aurora;Neo4j,Amazon Echo;AWS;iOS;Linux;Mac OS;Serverless,AWS;Linux;Mac OS;Serverless,Hadoop;Node.js;React;Spark,NA,IntelliJ;PyCharm;Sublime Text;Vim,MacOS,1,Agile;Evidence-based software engineering;Extreme programming (XP);Formal standard such as ISO 9001 or IEEE 12207 (aka waterfall methodologies);Kanban;Lean;Pair programming;Scrum,Git,Multiple times per day,No,NA,NA,Neither agree nor disagree,Somewhat agree,Neither agree nor disagree,Clicked on an online advertisement;Saw an online advertisement and then researched it (without clicking on the ad);Stopped going to a website because of their advertising,1,3,5,4,2,7,6,Algorithms making important decisions,\"Artificial intelligence surpassing human intelligence (\"\"the singularity\"\")\",The developers or the people creating the AI,I'm worried about the dangers more than I'm excited about the possibilities.,No,\"Yes, and publicly\",The person who came up with the idea,Yes,7,A few times per month or weekly,Yes,Less than once per month or monthly,Yes,Yes,7,No,Not at all interested,Not at all interested,Not at all interested,Not at all interested,Not at all interested,Between 8:01 - 9:00 AM,9 - 12 hours,Less than 30 minutes,1 - 2 times per week,Standing desk;Fatigue-relieving floor mat,I don't typically exercise,Male,Straight or heterosexual,Some college/university study without earning a degree,White or of European descent,35 - 44 years old,Yes,No,The survey was an appropriate length,Very easy\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en-wXmHrpy0f",
        "colab_type": "text"
      },
      "source": [
        "# TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgF9kYXKpznq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trace the model\n",
        "devices = [':{}'.format(n) for n in range(0, num_cores)]\n",
        "inputs = torch.zeros(batch_size, 1, 28, 28)\n",
        "target = torch.zeros(batch_size, dtype=torch.int64)\n",
        "xla_model = xm.XlaModel(model, [inputs], loss_fn=F.nll_loss, target=target, num_cores=num_cores, devices=devices)\n",
        "optimizer = optim.SGD(xla_model.parameters_list(), lr=lr, momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2yoLoinp2Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_fn = xm.get_log_fn()\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  xla_model.train(train_loader, optimizer, batch_size, log_interval=log_interval, metrics_debug=False,\n",
        "                  log_fn=log_fn)\n",
        "  accuracy = xla_model.test(test_loader, xm.category_eval_fn(F.nll_loss), batch_size, log_fn=log_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4nWxN3kHm_",
        "colab_type": "text"
      },
      "source": [
        "# ImageDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPyACpwvkI96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "    self.transform = Compose(transforms_)\n",
        "    \n",
        "    self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "    if mode == \"train\":\n",
        "      self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.files[index % len(self.files)])\n",
        "    w, h = img.size\n",
        "    img_A = img.crop((0, 0, w / 2, h))\n",
        "    img_B = img.crop((w / 2, 0, w, h))\n",
        "    \n",
        "    if np.random.random() < 0.5:\n",
        "      img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "      img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "    \n",
        "    img_A = self.transform(img_A)\n",
        "    img_B = self.transform(img_B)\n",
        "    \n",
        "    return {\"A\": img_A, \"B\": img_B}\n",
        "  \n",
        "  def __len__(self):\n",
        "    return (self.files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FIZNNYglYkr",
        "colab_type": "text"
      },
      "source": [
        "# FloatTensor, LongTensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckPRqPYUlacX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "Tensor = torch.cuda.FloatTensor\n",
        "# 2\n",
        "FloatTensor = torch.cuda.FloatTensor\n",
        "\n",
        "LongTensor = torch.cuda.LongTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyBmRJbGlrr8",
        "colab_type": "text"
      },
      "source": [
        "# Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EoOnZMls89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1\n",
        "def sample_images(batches_done):\n",
        "  \"\"\"\n",
        "  saves a generated sample from the validation set\n",
        "  \"\"\"\n",
        "# 2\n",
        "def sample_images(n_row, batches_done):\n",
        "  \"\"\"\n",
        "  saves a grid of generated digits ranging from 0 to n_classes\n",
        "  \"\"\"\n",
        "  # 1\n",
        "  imgs = next(iter(val_dataloader))\n",
        "  real_A = Variable(imgs[\"B\"].type(Tensor))\n",
        "  real_B = Variable(imgs[\"A\"].type(Tensor))\n",
        "  fake_B = generator(real_A)\n",
        "  img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
        "  save_image(img_sample, \"./%s/%s.png\" % (dataset_name, batches_done), nrow=5, normalize=True)\n",
        "  # 2\n",
        "  # sample noise\n",
        "  z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, latent_dim))))\n",
        "  # get labels ranging from 0 to n_classes for n rows\n",
        "  labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
        "  labels = Variable(LongTensor(labels))\n",
        "  gen_imgs = generator(z, labels)\n",
        "  save_image(gen_imgs.data, \"./%d.png\" % (batches_done), nrow=n_row, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zUxKXrqlxjP",
        "colab_type": "text"
      },
      "source": [
        "# transfer files between Google Colab and Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0y79kypKJ_J3",
        "colab": {}
      },
      "source": [
        "# copy files from google colab to google drive\n",
        "!cp -r omniglot_1_8_0.1_64_5_0/ ../gdrive/My\\ Drive/\n",
        "!cp -r runs/ ../gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5ohNe6LJ_JX",
        "colab": {}
      },
      "source": [
        "# retrieve files from google drive to google colab\n",
        "!cp -r ../gdrive/My\\ Drive/omniglot_1_8_0.1_64_5_0/ ./omniglot_1_8_0.1_64_5_0/\n",
        "!cp -r ../gdrive/My\\ Drive/runs/ ./runs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E31qKvdirci6",
        "colab_type": "text"
      },
      "source": [
        "# pep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvqHN4RMrd0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class c:\n",
        "  def __init_subclass__(cls, whom, **kwargs):\n",
        "    super().__init_subclass__(**kwargs)\n",
        "    cls.hello = lambda: print(f\"Hello, {whom}\")\n",
        "    \n",
        "class HelloWorld(c, whom=\"World\"):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRUNlbJZr8-I",
        "colab_type": "code",
        "outputId": "59cdaa3d-5bb1-464a-b582-90c30448985f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "HelloWorld.hello()\n",
        "HelloWorld."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, World\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHjktuHYt6G6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OrderedPreserved:\n",
        "  a = 1\n",
        "  b = 2\n",
        "  def meth(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz_RlGX7sOZn",
        "colab_type": "code",
        "outputId": "06ea0c6d-4dc1-4b7d-d192-70d4c142a735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(OrderedPreserved.__dict__.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__module__', 'a', 'b', 'meth', '__dict__', '__weakref__', '__doc__']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qMjjiuTuCoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX3CNsd5-YXv",
        "colab_type": "text"
      },
      "source": [
        "# train wGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCk8UGyt-aeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile('./model.tar'):\n",
        "  checkpoint = torch.load('model.tar')\n",
        "  generator.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "  discriminator.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "  optimizer_G.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "  optimizer_D.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "prev_time = time.time()\n",
        "iters = 0\n",
        "batches_done = 0\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (imgs, _) in enumerate(mnist_loader):\n",
        "    \n",
        "    # adversarial ground truths\n",
        "    # valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    # fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
        "    \n",
        "    # configure input\n",
        "    real_imgs = Variable(imgs.type(Tensor))\n",
        "    \n",
        "    # TRAIN DISCRIMINATOR\n",
        "    \n",
        "    optimizer_D.zero_grad()\n",
        "    \n",
        "    # sample noise as generator input\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "    \n",
        "    # generate a batch of images\n",
        "    fake_imgs = generator(z)\n",
        "    writer.add_images('fake_imgs', fake_imgs, iters)\n",
        "    \n",
        "    \n",
        "    # real images\n",
        "    real_validity = discriminator(real_imgs)\n",
        "    \n",
        "    # fake images\n",
        "    fake_validity = discriminator(fake_imgs)\n",
        "    \n",
        "    # gradient penalty\n",
        "    gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
        "    \n",
        "    # adversarial lloss\n",
        "    d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "    \n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "    \n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    # train the generator every n_critic steps\n",
        "    if i % n_critic == 0:\n",
        "      # TRAIN GENERATOR\n",
        "      \n",
        "      # generate a batch of images\n",
        "      fake_imgs = generator(z)\n",
        "      writer.add_images('fake_imgs_', fake_imgs, iters)\n",
        "      # loss measures generator's ability to fool teh discriminator\n",
        "      # train on fake images\n",
        "      fake_validity = discriminator(fake_imgs)\n",
        "      g_loss = -torch.mean(fake_validity)\n",
        "      \n",
        "      g_loss.backward()\n",
        "      optimizer_G.step()\n",
        "      \n",
        "      \n",
        "      print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % \n",
        "            (epoch, num_epochs, i, len(mnist_loader), d_loss.item(), g_loss.item()))\n",
        "      writer.add_scalar('d_loss', d_loss.item(), iters)\n",
        "      writer.add_scalar('g_loss', g_loss.item(), iters)\n",
        "      \n",
        "    \n",
        "      if batches_done % sample_interval == 0:\n",
        "        save_image(fake_imgs.data[:25], \"./%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "      \n",
        "      batches_done += n_critic\n",
        "    \n",
        "    \n",
        "    torch.save({\n",
        "        'modelA_state_dict': generator.state_dict(),\n",
        "        'modelB_state_dict': discriminator.state_dict(),\n",
        "        'optimizerA_state_dict': optimizer_G.state_dict(),\n",
        "        'optimizerB_state_dict': optimizer_D.state_dict()\n",
        "    }, 'model.tar')\n",
        "    \n",
        "    iters += 1\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZNVJpfBLM7",
        "colab_type": "text"
      },
      "source": [
        "# show_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74uAHJnBMUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_dataset(dataset, n=6):\n",
        "  img = np.vstack((np.hstack((np.asarray(dataset[i][0]) for _ in range(n))) for i in range(len(dataset))))\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTe9AF-3L8nh",
        "colab_type": "text"
      },
      "source": [
        "# imgaug"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvff-2Z4MWoi",
        "colab": {}
      },
      "source": [
        "# let us use imgaug\n",
        "!pip install git+https://github.com/aleju/imgaug\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "\n",
        "class ImgAugTransform:\n",
        "  def __init__(self):\n",
        "    self.aug = iaa.Sequential([\n",
        "        iaa.Resize((224, 224)),\n",
        "        iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
        "        iaa.Sometimes(0.25, iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
        "                                       iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
        "        iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
        "    ])\n",
        "    img = np.array(img)\n",
        "    return self.aug.augment_image(img)\n",
        "\n",
        "transforms = ImgAugTransform()\n",
        "\n",
        "aug = iaa.Affine(rotate=(-40, 40), mode='symmetric')\n",
        "imgs = [np.asarray(dataset[0][0]) for _ in range(6)]\n",
        "aug.augment_images(imgs)\n",
        "plt.imshow(np.hstack(imgs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn7ZOjmSMXze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mixing PyTorch and imgaug transforms\n",
        "\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    ImgAugTransform(),\n",
        "    lambda x: PIL.Image.fromarray(x),\n",
        "    torchvision.transforms.RandomVerticalFlip()\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms)\n",
        "\n",
        "show_dataset(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFTKZkx1Mh_Y",
        "colab_type": "text"
      },
      "source": [
        "# compare time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hjt_ECtMjgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = {\n",
        "    'pytorch': torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms_pytorch),\n",
        "    'imgaug' : torchvision.datasets.ImageFolder('pytorch-examples/data/', transform=transforms_imgaug)\n",
        "}\n",
        "\n",
        "times = {'pytorch': [], 'imgaug': []}\n",
        "for _ in range(20):\n",
        "  for mode in ('pytorch', 'imgaug'):\n",
        "    start = time.time()\n",
        "    img = np.vstack((np.hstack((np.asarray(datasets[mode][i][0]) for _ in range(6))) for i in range(4)))\n",
        "    end = time.time()\n",
        "    times[mode].append(end - start)\n",
        "\n",
        "for mode in ('pytorch', 'imgaug'):\n",
        "  t = np.array(times[mode])\n",
        "  print(\"{}: {:.04f}\".format(mode, t.min()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k7DxCOXIs6N",
        "colab_type": "text"
      },
      "source": [
        "# LambdaLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U41ZkD70IuZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LambdaLR:\n",
        "  def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "    assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "    self.n_epocsh = n_epochs\n",
        "    self.offset = offset\n",
        "    self.decay_start_epoch = decay_start_epoch\n",
        "  \n",
        "  def step(self, epoch):\n",
        "    return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpRYmXxjKHoY",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp08ZYunKI7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILRJbyzvKNxj",
        "colab_type": "text"
      },
      "source": [
        "# Average Meter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtWCLaV5KPyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9ZjryeuiRAD",
        "colab_type": "text"
      },
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_6F2-aFiSI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG16(torch.nn.Module):\n",
        "  def __init__(self, requires_grad=False):\n",
        "    super(VGG16, self).__init__()\n",
        "    vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "    \n",
        "    self.slice1 = torch.nn.Sequential()\n",
        "    self.slice2 = torch.nn.Sequential()\n",
        "    self.slice3 = torch.nn.Sequential()\n",
        "    self.slice4 = torch.nn.Sequential()\n",
        "    \n",
        "    for x in range(4):\n",
        "      self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(4, 9):\n",
        "      self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(9, 16):\n",
        "      self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "    for x in range(16, 23):\n",
        "      self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "    if not requires_grad:\n",
        "      for param in self.parameters():\n",
        "        param.requires_grad = False\n",
        "  \n",
        "  def forward(self, X):\n",
        "    h = self.slice1(X)\n",
        "    h_relu1_2 = h\n",
        "    h = self.slice2(h)\n",
        "    h_relu2_2 = h\n",
        "    h = self.slice3(h)\n",
        "    h_relu3_3 = h\n",
        "    h = self.slice4(h)\n",
        "    h_relu4_3 = h\n",
        "    vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n",
        "    out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "    return out    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1bTW6N7jHBW",
        "colab_type": "text"
      },
      "source": [
        "# ConvBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSh8pcD7jINi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True,\n",
        "               relu=True):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.upsample = upsample\n",
        "    self.block = nn.Sequential(\n",
        "        nn.ReflectionPad2d(kernel_size // 2),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "    )\n",
        "    self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n",
        "    self.relu = relu\n",
        "  \n",
        "  def forward(self, x):\n",
        "    if self.upsample:\n",
        "      x = F.interpolate(x, scale_factor=2)\n",
        "    x = self.block(x)\n",
        "    if self.norm is not None:\n",
        "      x = self.norm(x)\n",
        "    if self.relu:\n",
        "      x = F.relu(x)\n",
        "    return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBrn3pPDpBA4",
        "colab_type": "text"
      },
      "source": [
        "# denormalize, deprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvT2aRfupD2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denormalize(tensors):\n",
        "  \"\"\"\n",
        "  denormalizes image tensors using mean and std\n",
        "  \"\"\"\n",
        "  for c in range(3):\n",
        "    tensors[:, c].mul_(std[c]).add_(mean[c])\n",
        "  return tensors\n",
        "\n",
        "def deprocess(image_tensor):\n",
        "  \"\"\"\n",
        "  denormalizes and recales image tensor\n",
        "  \"\"\"\n",
        "  image_tensor = denormalize(image_tensor)[0]\n",
        "  image_tensor *= 255\n",
        "  image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.unit8)\n",
        "  image_np = image_np.transpose(1, 2, 0)\n",
        "  return image_np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9DkivUX3maJ",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c07PwADj3nPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, latent_dim, num_layers, hidden_dim, bidirecional):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "    self.hidden_state = None\n",
        "  \n",
        "  def reset_hidden_state(self):\n",
        "    self.hidden_state = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x, self.hidden_state = self.lstm(x, self.hidden_state)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxQr_TCM4cEg",
        "colab_type": "text"
      },
      "source": [
        "# Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wel0ZBPI4dvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, latent_dim, hidden_dim, attention_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.latent_attention = nn.Linear(latent_dim, attention_dim)\n",
        "    self.hidden_attention = nn.Linear(hidden_dim, attention_dim)\n",
        "    self.joint_attention = nn.Linear(attention_dim, 1)\n",
        "    \n",
        "  def forward(self, latent_repr, hidden_repr):\n",
        "    if hidden_repr is None:\n",
        "      hidden_repr = [\n",
        "          Variable(\n",
        "            torch.zeros(latent_repr.size(0), 1, self.hidden_attention.in_features), requires_grad=False\n",
        "          ).float()\n",
        "      ]\n",
        "    h_t = hidden_repr[0]\n",
        "    latent_att = self.latent_attention(latent_att)\n",
        "    hidden_att = self.hidden_attention(h_t)\n",
        "    joint_att = self.joint_attention(F.relu(latent_att + hidden_att)).squeeze(-1)\n",
        "    attention_w = F.softmax(joint_att, dim=-1)\n",
        "    return attention_w\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ClXz1Tr5jO3",
        "colab_type": "text"
      },
      "source": [
        "# ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYy6bami5kYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLSTM(nn.Module):\n",
        "  def __init__(self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True,\n",
        "               attention=True):\n",
        "    super(ConvLSTM, self).__init__()\n",
        "    self.encoder = Encoder(latent_dim)\n",
        "    self.lstm = LSTM(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
        "    self.output_layers = nn.Sequential(\n",
        "        nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, hidden_dim),\n",
        "        nn.BatchNorm1d(hidden_dim, momentum=0.01),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, num_classes),\n",
        "        nn.Softmax(dim=-1),\n",
        "    )\n",
        "    self.attention = attention\n",
        "    self.attention_layer = nn.Linear(2 * hidden_dim if bidirectional else hidden_dim, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    batch_size, seq_length, c, h, w = x.shape\n",
        "    x = x.view(batch_size * seq_length, c, h, w)\n",
        "    x = self.encoder(x)\n",
        "    x = x.view(batch_size, seq_length, -1)\n",
        "    x = self.lstm(x)\n",
        "    if self.attention:\n",
        "      attention_w = F.softmax(self.attention_layer(x).squeeze(-1), dim=-1)\n",
        "      x = torch.sum(attention_w.unsqueeze(-1) * x, dim=1)\n",
        "    else:\n",
        "      x = x[:, -1]\n",
        "    return self.output_layers(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwOKCMwq9qle",
        "colab_type": "text"
      },
      "source": [
        "# mean, std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX4PWA0i9r8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8DD0EqDRHAu",
        "colab_type": "text"
      },
      "source": [
        "# history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBFIm0omRH47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%history -n 1-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snwWwDoMi5r0",
        "colab_type": "text"
      },
      "source": [
        "# LaTeX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfBEaodti6wA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Math, HTML\n",
        "\n",
        "display(HTML(\"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/\"\n",
        "             \"latest.js?config=default'></script>\"))\n",
        "\n",
        "Math('\\Delta = b^2 - 4ac')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPh5ty_JXv1C",
        "colab_type": "text"
      },
      "source": [
        "# conv1x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKZ4aQ_MXx5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "  \"\"\"\n",
        "  1x1 convolution\n",
        "  \"\"\"\n",
        "  return nn.Conv2d(in_planese, out_planes, kernel_size, stride=stride, bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPrsoeO2Xyff",
        "colab_type": "text"
      },
      "source": [
        "# conv3x3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BewTi34IXzc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "  \"\"\"\n",
        "  3x3 dilation with padding\n",
        "  \"\"\"\n",
        "  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups,\n",
        "                   bias=False, dilation=dilation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdpzSJlBZF-z",
        "colab_type": "text"
      },
      "source": [
        "# BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceZJkCIaZHHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "  \n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1,\n",
        "               norm_layer=None):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    if norm_layer is None:\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "    \n",
        "    if groups != 1 or base_width != 64:\n",
        "      raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "    \n",
        "    if dilation > 1:\n",
        "      raise NotImplementedError('Dilation > 1 not supported in BasicBlock')\n",
        "    \n",
        "    # both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "    self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "    self.bn1 = norm_layer(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(planes, planes)\n",
        "    self.bn2 = norm_layer(planes)\n",
        "    self.downsample = downsample\n",
        "    self.stride = stride\n",
        "  \n",
        "  def forward(self, x):\n",
        "    identity = x\n",
        "    \n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    \n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    \n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "    \n",
        "    out += identity\n",
        "    out = self.relu(out)\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn0_mlDq5KIu",
        "colab_type": "text"
      },
      "source": [
        "# print_network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHRBLYbM5LS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_network(net):\n",
        "  num_params = 0\n",
        "  for param in net.parameters():\n",
        "    num_params += param.numel()\n",
        "  \n",
        "  print(net)\n",
        "  print('Total number of parameters: %d' % num_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMr05IMkKMq7",
        "colab_type": "text"
      },
      "source": [
        "# hsigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drT-XxRzKNcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class hsigmoid(nn.Module):\n",
        "  def forward(self, x):\n",
        "    out = F.relu6(x + 3, inplace=True) / 6\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xITJMsSMkhSq",
        "colab_type": "text"
      },
      "source": [
        "# ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoinB2GFkifi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classes: ResNetLike\n",
        "\n",
        "# model: ResNetLike\n",
        "# input: one query image and a support set\n",
        "# base_model: 4 ResBlock layers -> Image-to-Class layer\n",
        "# dataset: 3 x 84 x 84, for miniImageNet\n",
        "# filters: 64 -> 96 -> 128 -> 256\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, nFin, nFout):\n",
        "    super(ResBlock, self).__init__()\n",
        "    \n",
        "    self.conv_block = nn.Sequential()\n",
        "    self.conv_block.add_module('BNorm1', nn.BatchNorm2d(nFin))\n",
        "    self.conv_block.add_module('LRelu1', nn.LeakyReLU(0.2))\n",
        "    self.conv_block.add_module('ConvL1', nn.Conv2d(nFin, nFout, kernel_size=3, padding=1, bias=False))\n",
        "    self.conv_block.add_module('BNorm2', nn.BatchNorm2d(nFout))\n",
        "    self.conv_block.add_module('LRelu2', nn.LeakyReLU(0.2))\n",
        "    self.conv_block.add_module('ConvL2', nn.Conv2d(nFout, nFout, kernel_size=3, padding=1, bias=False))\n",
        "    self.conv_block.add_module('BNorm3', nn.BatchNorm2d(nFout))\n",
        "    self.conv_block.add_module('LRelu3', nn.LeakyReLU(0.2))\n",
        "    self.conv_block.add_module('ConvL3', nn.Conv2d(nFout, nFout, kernel_size=3, padding=1, bias=False))\n",
        "    \n",
        "    self.skip_layer = nn.Conv2d(nFin, nFout, kernel_size=1, stride=1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.skip_layer(x) + self.conv_block(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXAS6fUMGU5f",
        "colab_type": "text"
      },
      "source": [
        "# LambdaTransforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmiR5268GWh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TenCrop(224),\n",
        "Lambda(lambda crops: torch.stack([normalize(transforms.ToTensor()(crop)) for crop in crops]))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}